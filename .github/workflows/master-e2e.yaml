# This workflow is a reusable one called by other workflows
name: (template) Elemental E2E tests with Rancher Manager

on:
  workflow_call:
    # Variables to set when calling this reusable workflow
    secrets:
      credentials:
        description: Credentials to use to connect
        required: true
      pat_token:
        # A token is needed to be able to add runner on the repo, maybe this can be changed later
        # This token is linked to a personal account
        # So in case of token issue you have to check (no specific order and for example):
        # - the expiration date
        # - if the account associated still exists
        # - if the person still has access to the repo
        description: PAT token used to add runner
        required: true
      qase_api_token:
        description: Qase API token to use for Cypress tests
        required: false
      slack_webhook_url:
        description: WebHook URL to use for Slack
        required: true
    inputs:
      backup_restore_version:
        description: Version of backup-restore-operator to use
        type: string
      qase_run_id:
        description: Case run ID where the results will be reported
        required: false
        type: string
      ca_type:
        description: CA type to use (selfsigned or private)
        default: selfsigned
        type: string
      cert-manager_version:
        description: Version of cert-manager to use
        type: string
      cluster_name:
        description: Name of the provisioned cluster
        required: true
        type: string
      cluster_number:
        description: Number of clusters to deploy in multi-cluster test
        type: string
      cluster_type:
        description: Cluster type (empty if normal or hardened)
        type: string
      cypress_tags:
        description: Tags to filter tests we want to run
        default: main
        type: string
      destroy_runner:
        description: Destroy the auto-generated self-hosted runner
        default: true
        type: boolean
      elemental_support:
        description: URL of the elemental support binary
        default: https://github.com/rancher/elemental-operator/releases/download/v1.1.4/elemental-support_1.1.4_linux_amd64
        type: string
      elemental_ui_version:
        description: Version of the elemental ui which will be installed (dev/stable)
        default: dev
        type: string
      iso_boot:
        description: Choose booting from ISO
        default: false
        type: boolean
      k8s_version_to_provision:
        description: Name and version of installed K8s distribution
        required: true
        type: string
      node_number:
        description: Number of nodes to deploy on the provisioned cluster
        default: 5
        type: string
      operator_repo:
        description: Elemental operator repository to use
        type: string
        default: oci://registry.opensuse.org/isv/rancher/elemental/dev/charts/rancher
      operator_upgrade:
        description: Elemental operator version to upgrade to
        type: string
      os_to_test:
        description: OS repository to test (dev/staging/stable)
        type: string
        default: dev
      proxy:
        description: Deploy a proxy
        type: string
      rancher_log_collector:
        description: URL of the Rancher log collector script
        default: https://raw.githubusercontent.com/rancherlabs/support-tools/master/collection/rancher/v2.x/logs-collector/rancher2_logs_collector.sh
        type: string
      rancher_version:
        description: Rancher Manager channel/version/head_version to use for installation
        default: stable/latest/none
        type: string
      rancher_upgrade:
        description: Rancher Manager channel/version to upgrade to
        type: string
      reset:
        description: Allow reset test (mainly used on CLI tests)
        default: false
        type: boolean
      runner_template:
        description: Runner template to use
        default: elemental-e2e-ci-runner-spot-x86-64-template-n2-standard-16-v5
        type: string
      sequential:
        description: Defines if bootstrapping is done sequentially (true) or in parallel (false)
        default: false
        type: boolean
      test_description:
        description: Short description of the test
        type: string
      test_type:
        description: Type of test to run (cli or ui)
        default: single_cli
        type: string
      ui_account:
        description: Account used to test RBAC role in UI
        required: false
        type: string
      upgrade_image:
        description: Image to use for the Elemental OS upgrade
        type: string
      upgrade_os_channel:
        description: Channel to use for the Elemental OS upgrade
        type: string
      upgrade_type:
        description: Type of upgrade to use for the Elemental OS upgrade
        type: string
      upstream_cluster_version:
        description: Cluster upstream version where to install Rancher (K3s or RKE2)
        default: v1.26.10+k3s2
        type: string
      zone:
        description: GCP zone to host the runner
        default: us-central1-a
        type: string

jobs:
  create-runner:
    runs-on: ubuntu-latest
    outputs:
      uuid: ${{ steps.generator.outputs.uuid }}
      runner: ${{ steps.generator.outputs.runner }}
      public_dns: ${{ steps.dns.outputs.public_dns }}
    steps:
      # actions/checkout MUST come before auth
      - name: Checkout
        uses: actions/checkout@v3
      - name: Generate UUID and Runner hostname
        id: generator
        run: |
          # NOTE: keep the runner name to less than 63 characters!
          UUID=$(uuidgen)
          GH_REPO_FULL=${{ github.repository }}
          GH_REPO=${GH_REPO_FULL#*/}
          echo "uuid=${UUID//-}" >> ${GITHUB_OUTPUT}
          echo "runner=${GH_REPO//\//-}-ci-${UUID//-}" >> ${GITHUB_OUTPUT}
      - name: Authenticate to GCP
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.credentials }}
      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v1
      - name: Create runner
        run: |
          gcloud compute instances create ${{ steps.generator.outputs.runner }} \
            --source-instance-template ${{ inputs.runner_template }} \
            --zone ${{ inputs.zone }}
      - name: Create GCP secrets
        run: |
          echo -n ${{ secrets.pat_token }} \
            | gcloud secrets create PAT_TOKEN_${{ steps.generator.outputs.uuid }} --data-file=-
          echo -n ${{ github.repository }} \
            | gcloud secrets create GH_REPO_${{ steps.generator.outputs.uuid }} --data-file=-
      - name: Get public dns name in GCP
        id: dns
        run: |
          # Do a timed out loop here, as gcloud can sometimes fail
          typeset -i i=0
          while true; do
            # Get public IP
            PUBLIC_IP=$(gcloud compute instances list 2> /dev/null \
                        | awk '/${{ steps.generator.outputs.runner }}/ {print $6}')
            # Exit if we reach the timeout or if IP is set
            if (( ++i > 10 )) || [[ -n "${PUBLIC_IP}" ]]; then
              break
            fi
            # Wait a little before retrying
            sleep 2
          done
          # Get the public DNS
          PUBLIC_DNS=$(host -l ${PUBLIC_IP} 2> /dev/null \
                       | awk '{sub(/\.$/, ""); print $5}')
          echo "public_dns=${PUBLIC_DNS}" >> ${GITHUB_OUTPUT}
          # Raise an error if either IP and/or DNS are empty
          if [[ -z "${PUBLIC_IP}" || -z "${PUBLIC_DNS}" ]]; then
            echo "PUBLIC_IP and/or PUBLIC_DNS are empty!" >&2
            false
          fi
  e2e:
    needs: create-runner
    runs-on: ${{ needs.create-runner.outputs.uuid }}
    outputs:
      qase_run_id: ${{ steps.qase.outputs.qase_run_id }}
      steps_status: ${{ join(steps.*.conclusion, ' ') }}
    env:
      ARCH: amd64
      CERT_MANAGER_VERSION: ${{ inputs.cert-manager_version }}
      CLUSTER_NAME: ${{ inputs.cluster_name }}
      CLUSTER_NS: fleet-default
      CLUSTER_TYPE: ${{ inputs.cluster_type }}
      # K3S / RKE2 flags to use for installation
      INSTALL_K3S_SKIP_ENABLE: true
      INSTALL_K3S_VERSION: ${{ inputs.upstream_cluster_version }}
      INSTALL_RKE2_VERSION: ${{ inputs.upstream_cluster_version }}
      K3S_KUBECONFIG_MODE: 0644
      # Distribution to use to host Rancher Manager (K3s or RKE2)
      K8S_UPSTREAM_VERSION: ${{ inputs.upstream_cluster_version }}
      # For K8s cluster to provision with Rancher Manager
      K8S_VERSION_TO_PROVISION: ${{ inputs.k8s_version_to_provision }}
      # QASE variables
      QASE_API_TOKEN: ${{ secrets.qase_api_token }}
      QASE_PROJECT_CODE: ELEMENTAL
      QASE_REPORT: 1
      QASE_RUN_COMPLETE: 1
      QASE_RUN_DESCRIPTION: TO_BE_CHANGED
      QASE_RUN_ID: ${{ inputs.qase_run_id }}
      QASE_RUN_NAME: TO_BE_CHANGED
      # For Rancher Manager
      RANCHER_VERSION: ${{ inputs.rancher_version }}
      TEST_TYPE: ${{ inputs.test_type }}
      TIMEOUT_SCALE: 3
    steps:
      - name: Checkout
        id: checkout
        uses: actions/checkout@v3
      - name: Install Go
        id: install_go
        uses: actions/setup-go@v3
        with:
          go-version-file: tests/go.mod
      - name: Define needed system variables
        id: define_sys_vars
        run: |
          # Add missing PATH, removed in recent distributions for security reasons...
          echo "/usr/local/bin" >> ${GITHUB_PATH}
      - name: Deploy Proxy
        id: proxy
        if: ${{ inputs.proxy == 'elemental' || inputs.proxy == 'rancher' }}
        run: docker run -d --name squid_proxy -v $(pwd)/tests/assets/squid.conf:/etc/squid/squid.conf -p 3128:3128 wernight/squid
      - name: Create Qase Run (if needed)
        id: qase
        if: ${{ inputs.qase_run_id == 'auto' }}
        run: |
          # Export the Qase run name, as it cannot be done in
          # 'env:' because GITHUB_WORKFLOW is a shell variable
          echo "QASE_RUN_NAME=${GITHUB_WORKFLOW}" >> ${GITHUB_ENV}

          # Also export URL of GH test run in Qase run description
          GH_RUN_URL="${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
          QASE_DESC="${{ inputs.test_description }} (${GH_RUN_URL})"
          echo "QASE_RUN_DESCRIPTION=${QASE_DESC}" >> ${GITHUB_ENV}

          # Export them also to be used locally
          export QASE_RUN_NAME="${GITHUB_WORKFLOW}"
          export QASE_RUN_DESCRIPTION="${QASE_DESC}"

          # We need to always unset the faking ID variable
          unset QASE_RUN_ID

          ID=$(cd tests && make create-qase-run)
          if [[ -n "${ID}" ]]; then
            # Export QASE_RUN_ID to be sure that we always have the same value
            echo "QASE_RUN_ID=${ID}" >> ${GITHUB_ENV}
            export QASE_RUN_ID=${ID}
          fi

          # Output for the clean step
          echo "qase_run_id=${QASE_RUN_ID}" >> ${GITHUB_OUTPUT}

          # Just an info for debugging purposes
          echo "Export QASE_RUN_ID=${QASE_RUN_ID}, QASE_RUN_DESCRIPTION=${QASE_RUN_DESCRIPTION} and QASE_RUN_NAME=${QASE_RUN_NAME}"
      - name: Install Rancher+Elemental components
        id: install_rancher_elemental
        env:
          CA_TYPE: ${{ inputs.ca_type }}
          OPERATOR_REPO: ${{ inputs.operator_repo }}
          PROXY: ${{ inputs.proxy }}
          PUBLIC_DNS: ${{ needs.create-runner.outputs.public_dns }}
          PUBLIC_DOMAIN: bc.googleusercontent.com
        run: cd tests && make e2e-install-rancher
      - name: Workaround for DynamicSchemas (if needed)
        run: |
          # Check if DynamicSchemas for MachineInventorySelectorTemplate exists
          if ! kubectl get dynamicschema machineinventoryselectortemplate >/dev/null 2>&1; then
            # If not we have to add it to avoid weird issues!
            echo "WORKAROUND: DynamicSchemas for MachineInventorySelectorTemplate is missing!"
            kubectl apply -f tests/assets/add_missing_dynamicschemas.yaml
          fi
      - name: Install backup-restore components (K3s only for now)
        id: install_backup_restore
        if: ${{ inputs.test_type == 'single_cli' && contains(inputs.upstream_cluster_version, 'k3s') }}
        run: cd tests && make e2e-install-backup-restore
      - name: Extract component versions/informations
        id: component
        run: |
          # Extract rancher-backup-operator version
          BACKUP_RESTORE_VERSION=$(kubectl get pod \
                                     --namespace cattle-resources-system \
                                     -l app.kubernetes.io/name=rancher-backup \
                                     -o jsonpath={.items[*].status.containerStatuses[*].image} 2> /dev/null || true)
          # Extract CertManager version
          CERT_MANAGER_VERSION=$(kubectl get pod \
                                   --namespace cert-manager \
                                   -l app=cert-manager \
                                   -o jsonpath={.items[*].status.containerStatuses[*].image} 2> /dev/null || true)
          # Extract elemental-operator version
          OPERATOR_VERSION=$(kubectl get pod \
                             --namespace cattle-elemental-system \
                             -l app=elemental-operator \
                             -o jsonpath={.items[*].status.containerStatuses[*].image} 2> /dev/null || true)
          # Extract Rancher Manager version
          RM_VERSION=$(kubectl get pod \
                         --namespace cattle-system \
                         -l app=rancher \
                         -o jsonpath={.items[*].status.containerStatuses[*].image} 2> /dev/null || true)
          # Export values
          echo "backup_restore_version=${BACKUP_RESTORE_VERSION}" >> ${GITHUB_OUTPUT}
          echo "cert_manager_version=${CERT_MANAGER_VERSION}" >> ${GITHUB_OUTPUT}
          echo "operator_version=${OPERATOR_VERSION}" >> ${GITHUB_OUTPUT}
          echo "rm_version=${RM_VERSION}" >> ${GITHUB_OUTPUT}
      - name: Install Chartmuseum
        id: install_chartmuseum
        if: ${{ inputs.test_type == 'ui' }}
        run: cd tests && make e2e-install-chartmuseum
      - name: Cypress tests - Basics
        id: cypress_basics
        # Basics means tests without an extra elemental node needed
        if: ${{ inputs.test_type == 'ui' }}
        env:
          BROWSER: chrome
          CHARTMUSEUM_REPO: http://${{ needs.create-runner.outputs.public_dns }}
          CYPRESS_DOCKER: 'cypress/included:10.9.0'
          CYPRESS_TAGS: ${{ inputs.cypress_tags }}
          ELEMENTAL_UI_VERSION: ${{ inputs.elemental_ui_version }}
          ISO_BOOT: ${{ inputs.iso_boot }}
          K8S_UPSTREAM_VERSION: ${{ inputs.upstream_cluster_version }}
          OPERATOR_REPO: ${{ inputs.operator_repo }}
          RANCHER_VERSION: ${{ steps.component.outputs.rm_version }}
          RANCHER_PASSWORD: rancherpassword
          RANCHER_URL: https://${{ needs.create-runner.outputs.public_dns }}/dashboard
          RANCHER_USER: admin
          SPEC: |
            /workdir/e2e/unit_tests/first_connection.spec.ts
            /workdir/e2e/unit_tests/elemental_operator.spec.ts
            /workdir/e2e/unit_tests/elemental_plugin.spec.ts
            /workdir/e2e/unit_tests/user.spec.ts
            /workdir/e2e/unit_tests/menu.spec.ts
            /workdir/e2e/unit_tests/machine_registration.spec.ts
            /workdir/e2e/unit_tests/advanced_filtering.spec.ts
          UI_ACCOUNT: ${{ inputs.ui_account }}
          UPGRADE_OS_CHANNEL: ${{ inputs.upgrade_os_channel }}
        run: cd tests && make start-cypress-tests
      - name: Upload Cypress screenshots (Basics)
        id: upload_screenshots_cypress_basics
        if: ${{ failure() && inputs.test_type == 'ui' }}
        uses: actions/upload-artifact@v3
        with:
          name: cypress-screenshots-basics-${{ inputs.cluster_name }}
          path: tests/cypress/latest/screenshots
          retention-days: 7
          if-no-files-found: ignore
      - name: Upload Cypress videos (Basics)
        id: upload_videos_cypress_basics
        # Test run video is always captured, so this action uses "always()" condition
        if: ${{ always() && inputs.test_type == 'ui' }}
        uses: actions/upload-artifact@v3
        with:
          name: cypress-videos-basics-${{ inputs.cluster_name }}
          path: tests/cypress/latest/videos
          retention-days: 7
      - name: Deploy a node to join Rancher manager
        id: deploy_node_ui
        if: ${{ inputs.test_type == 'ui' }}
        env:
          ISO_BOOT: ${{ inputs.iso_boot }} 
          VM_INDEX: 1
          VM_MEM: 8192
          HOST_MEMORY_RESERVED: 49152
        run: |
          cd tests && (
            # Removing 'downloads' is needed to avoid this error during 'make':
            # 'pattern all: open .../elemental/tests/cypress/downloads: permission denied'
            sudo rm -rf cypress/latest/downloads
            make e2e-ui-rancher
          )
      - name: Cypress tests - Advanced
        id: cypress_advanced
        # Advanced means tests which needs an extra elemental node (provisioned with libvirt)
        if: ${{ inputs.test_type == 'ui' }}
        env:
          BROWSER: firefox
          CHARTMUSEUM_REPO: http://${{ needs.create-runner.outputs.public_dns }}
          CYPRESS_DOCKER: 'cypress/included:10.9.0'
          CYPRESS_TAGS: ${{ inputs.cypress_tags }}
          ELEMENTAL_UI_VERSION: ${{ inputs.elemental_ui_version }}
          OPERATOR_REPO: ${{ inputs.operator_repo }}
          PROXY: ${{ inputs.proxy }}
          RANCHER_VERSION: ${{ steps.component.outputs.rm_version }}
          RANCHER_PASSWORD: rancherpassword
          RANCHER_URL: https://${{ needs.create-runner.outputs.public_dns }}/dashboard
          RANCHER_USER: admin
          SPEC: |
            /workdir/e2e/unit_tests/machine_selector.spec.ts
            /workdir/e2e/unit_tests/machine_inventory.spec.ts
            /workdir/e2e/unit_tests/reset.spec.ts
            /workdir/e2e/unit_tests/deploy_app.spec.ts
            /workdir/e2e/unit_tests/upgrade-operator.spec.ts
            /workdir/e2e/unit_tests/upgrade-ui-extension.spec.ts
            /workdir/e2e/unit_tests/upgrade.spec.ts
          UI_ACCOUNT: ${{ inputs.ui_account }}
          UPGRADE_IMAGE: ${{ inputs.upgrade_image }}
          UPGRADE_OS_CHANNEL: ${{ inputs.upgrade_os_channel }}
        run: cd tests && make start-cypress-tests
      - name: Upload Cypress screenshots (Advanced)
        id: upload_screenshots_cypress_advanced
        if: ${{ failure() && inputs.test_type == 'ui' }}
        uses: actions/upload-artifact@v3
        with:
          name: cypress-screenshots-advanced-${{ inputs.cluster_name }}
          path: tests/cypress/latest/screenshots
          retention-days: 7
          if-no-files-found: ignore
      - name: Upload Cypress videos (Advanced)
        id: upload_videos_cypress_advanced
        # Test run video is always captured, so this action uses "always()" condition
        if: ${{ always() && inputs.test_type == 'ui' }}
        uses: actions/upload-artifact@v3
        with:
          name: cypress-videos-advanced-${{ inputs.cluster_name }}
          path: tests/cypress/latest/videos
          retention-days: 7
      - name: Configure Rancher & Libvirt
        id: configure_rancher
        if: ${{ inputs.test_type == 'single_cli' }}
        run: cd tests && make e2e-configure-rancher
      - name: Create ISO image for master pool
        id: create_iso_master
        if: ${{ inputs.test_type == 'single_cli' }}
        env:
          EMULATE_TPM: true
          OS_TO_TEST: ${{ inputs.os_to_test }}
          POOL: master
        run: |
          # Only use ISO boot if the upstream cluster is RKE2
          # due to issue with pxe, dhcp traffic
          if ${{ contains(inputs.upstream_cluster_version, 'rke') }}; then
            export ISO_BOOT=true
          fi
          cd tests && make e2e-iso-image
      - name: Extract iPXE artifacts from ISO
        id: extract_ipxe_artifacts
        if: ${{ inputs.test_type == 'single_cli' && inputs.iso_boot == false }}
        run: cd tests && make extract_kernel_init_squash && make ipxe
      - name: Bootstrap node 1, 2 and 3 in pool "master" (use Emulated TPM if possible)
        id: bootstrap_master_nodes
        if: ${{ inputs.test_type == 'single_cli' }}
        env:
          EMULATE_TPM: true
          POOL: master
          VM_START: 1
          VM_END: 3
        run: |
          # Only use ISO boot if the upstream cluster is RKE2
          # due to issue with pxe, dhcp traffic
          # Set RAM to 10GB for RKE2 and vCPU to 6, a bit more than the recommended values
          if ${{ contains(inputs.upstream_cluster_version, 'rke') }}; then
            export ISO_BOOT=true
            export VM_MEM=10240
            export VM_CPU=6
          fi
          # Execute bootstrapping test
          if ${{ inputs.sequential == true }}; then
            # Force node bootstrapping in sequential instead of parallel
            cd tests
            for ((i = VM_START ; i <= VM_END ; i++)); do
              VM_INDEX=${i} make e2e-bootstrap-node
            done
          else
            cd tests && VM_INDEX=${VM_START} VM_NUMBERS=${VM_END} make e2e-bootstrap-node
          fi
      - name: Deploy multiple clusters (with 3 nodes by cluster)
        id: deploy_multi_clusters
        if: inputs.test_type == 'multi_cli'
        env:
          CLUSTER_NUMBER: ${{ inputs.cluster_number }}
          ISO_BOOT: ${{ inputs.iso_boot }}
        run: |
          # Set RAM to 10GB for RKE2 and vCPU to 6, a bit more than the recommended values
          if ${{ contains(inputs.upstream_cluster_version, 'rke') }}; then
            export VM_MEM=10240
            export VM_CPU=6
          fi
          cd tests && make e2e-multi-cluster
      - name: Install a simple application
        id: install_simple_app
        if: ${{ inputs.test_type == 'single_cli' && contains(inputs.upstream_cluster_version, 'k3s') }}
        run: cd tests && make e2e-install-app && make e2e-check-app
      - name: Reset a node in the cluster
        id: reset_node
        if: ${{ inputs.test_type == 'single_cli' && inputs.reset == true }}
        run: cd tests && make e2e-reset
      - name: Check app after reset
        id: check_app
        if: ${{ inputs.test_type == 'single_cli' && inputs.reset == true && contains(inputs.upstream_cluster_version, 'k3s') }}
        run: cd tests && make e2e-check-app
      - name: Upgrade Elemental Operator
        id: operator_upgrade
        if: ${{ inputs.test_type == 'single_cli' && inputs.operator_upgrade != '' }}
        env:
          OPERATOR_UPGRADE: ${{ inputs.operator_upgrade }}
        run: |
          cd tests && make e2e-upgrade-operator
          if ${{ contains(inputs.upstream_cluster_version, 'k3s') }}; then
            make e2e-check-app
          fi
          # Extract elemental-operator version
          OPERATOR_VERSION=$(kubectl get pod \
                             --namespace cattle-elemental-system \
                             -l app=elemental-operator \
                             -o jsonpath={.items[*].status.containerStatuses[*].image} 2> /dev/null || true)
          # Export values
          echo "operator_upgrade=${OPERATOR_UPGRADE}" >> ${GITHUB_OUTPUT}
          echo "operator_version=${OPERATOR_VERSION}" >> ${GITHUB_OUTPUT}
      - name: Upgrade Rancher Manager
        id: rancher_upgrade
        if: ${{ inputs.test_type == 'single_cli' && inputs.rancher_upgrade != '' }}
        env:
          CA_TYPE: ${{ inputs.ca_type }}
          PROXY: ${{ inputs.proxy }}
          PUBLIC_DNS: ${{ needs.create-runner.outputs.public_dns }}
          PUBLIC_DOMAIN: bc.googleusercontent.com
          RANCHER_UPGRADE: ${{ inputs.rancher_upgrade }}
        run: |
          cd tests && make e2e-upgrade-rancher-manager
          if ${{ contains(inputs.upstream_cluster_version, 'k3s') }}; then
            make e2e-check-app
          fi
          # Extract Rancher Manager version
          RM_VERSION=$(kubectl get pod \
                         --namespace cattle-system \
                         -l app=rancher \
                         -o jsonpath={.items[*].status.containerStatuses[*].image} 2> /dev/null || true)
          # Export values
          echo "rm_version=${RM_VERSION}" >> ${GITHUB_OUTPUT}
      - name: Upgrade node 1 to specified OS version with osImage
        id: upgrade_node_1
        if: ${{ inputs.test_type == 'single_cli' && inputs.upgrade_image != '' }}
        env:
          UPGRADE_IMAGE: ${{ inputs.upgrade_image }}
          UPGRADE_TYPE: osImage
          VM_INDEX: 1
        run: |
          cd tests && make e2e-upgrade-node
          if ${{ contains(inputs.upstream_cluster_version, 'k3s') }}; then
            make e2e-check-app
          fi
      - name: Upgrade other nodes to specified OS version with managedOSVersionName
        id: upgrade_other_nodes
        if: ${{ inputs.test_type == 'single_cli' && inputs.upgrade_os_channel != '' }}
        env:
          UPGRADE_OS_CHANNEL: ${{ inputs.upgrade_os_channel }}
          UPGRADE_TYPE: managedOSVersionName
          VM_INDEX: 2
          VM_NUMBERS: 3
        run: |
          cd tests && make e2e-upgrade-node
          if ${{ contains(inputs.upstream_cluster_version, 'k3s') }}; then
            make e2e-check-app
          fi
      - name: Test Backup/Restore Elemental resources with Rancher Manager
        id: test_backup_restore
        if: ${{ inputs.test_type == 'single_cli' && contains(inputs.upstream_cluster_version, 'k3s') }}
        env:
          BACKUP_RESTORE_VERSION: ${{ inputs.backup_restore_version }}
        run: |
          cd tests && make e2e-backup-restore
          if ${{ contains(inputs.upstream_cluster_version, 'k3s') }}; then
            make e2e-check-app
          fi
      - name: Extract ISO version
        id: iso_version
        if: ${{ always() }}
        run: |
          # Extract OS version from ISO
          ISO=$(file -Ls *.iso 2>/dev/null | awk -F':' '/boot sector/ { print $1 }')
          if [[ -n "${ISO}" ]]; then
            INITRD_FILE=$(isoinfo -i ${ISO} -R -find -type f -name initrd -print 2>/dev/null)
            isoinfo -i ${ISO} -R -x ${INITRD_FILE} 2>/dev/null \
              | xz -dc \
              | cpio -i --to-stdout usr/lib/initrd-release > os-release
            eval $(grep IMAGE_TAG os-release 2>/dev/null)
          fi
          # Export value (even if empty!)
          echo "image_tag=${IMAGE_TAG}" >> ${GITHUB_OUTPUT}
      - name: Remove old built ISO image
        id: clean_master_iso
        # Only one at a time is allowed, the new one will be created after if needed
        if: ${{ inputs.test_type == 'single_cli' }}
        run: rm -f *.iso
      - name: Create ISO image for worker pool
        if: ${{ inputs.test_type == 'single_cli' }}
        env:
          ISO_BOOT: true
          OS_TO_TEST: ${{ inputs.os_to_test }}
          POOL: worker
        run: cd tests && make e2e-iso-image
      - name: Bootstrap additional nodes in pool "worker" (total of ${{ inputs.node_number }})
        id: bootstrap_worker_nodes
        if: ${{ inputs.test_type == 'single_cli' && inputs.node_number > 3 }}
        env:
          ISO_BOOT: true
          POOL: worker
          VM_START: 4
          VM_END: ${{ inputs.node_number }}
        run: |
          # Set RAM to 10GB for RKE2 and vCPU to 6, a bit more than the recommended values
          if ${{ contains(inputs.upstream_cluster_version, 'rke') }}; then
            export VM_MEM=10240
            export VM_CPU=6
          fi
          if ${{ inputs.sequential == true }}; then
            # Force node bootstrapping in sequential instead of parallel
            cd tests
            for ((i = VM_START ; i <= VM_END ; i++)); do
              VM_INDEX=${i} make e2e-bootstrap-node
            done
          else
            cd tests && VM_INDEX=${VM_START} VM_NUMBERS=${VM_END} make e2e-bootstrap-node
          fi
          # Check the installed application
          if ${{ contains(inputs.upstream_cluster_version, 'k3s') }}; then
            make e2e-check-app
          fi
      - name: Uninstall Elemental Operator
        id: uninstall_elemental_operator
        env:
          OPERATOR_REPO: ${{ inputs.operator_repo }}
        # Don't test Operator uninstall if we want to keep the runner for debugging purposes
        if: ${{ inputs.destroy_runner == true && inputs.test_type == 'single_cli'  }}
        run: cd tests && make e2e-uninstall-operator
      - name: Get logs
        id: get_logs
        if: ${{ always() }}
        env:
          ELEMENTAL_SUPPORT: ${{ inputs.elemental_support }}
          PROXY: ${{ inputs.proxy }}
          RANCHER_LOG_COLLECTOR: ${{ inputs.rancher_log_collector }}
        run: |
          cd tests && (
            # Removing 'downloads' is needed to avoid this error during 'make':
            # 'pattern all: open .../elemental/tests/cypress/downloads: permission denied'
            sudo rm -rf cypress/latest/downloads
            make e2e-get-logs
          )
      - name: Upload logs
        id: uploade_logs
        if: ${{ always() }}
        uses: actions/upload-artifact@v3
        with:
          name: support-logs
          path: tests/**/logs/*
          if-no-files-found: ignore
      - name: Add summary
        id: add_summary
        if: ${{ always() }}
        run: |
          # Define some variable(s)
          BOOTSTRAP_METHOD="Parallel"
          if ${{ inputs.sequential == true }}; then
            BOOTSTRAP_METHOD="Sequential"
          fi
          # Get nodes configuration (use the first one, they are all identical)
          NODE=$(sudo virsh list --name | head -1)
          if [[ -n "${NODE}" ]]; then
            VCPU=$(sudo virsh vcpucount --live ${NODE})
            VMEM=$(sudo virsh dommemstat --live ${NODE} | awk '/^actual/ { print $2 }')
            (( VMEM /= 1048576 ))
          fi
          # Add summary
          echo "## General informations" >> ${GITHUB_STEP_SUMMARY}
          echo -e "***${{ inputs.test_description }}***\n" >> ${GITHUB_STEP_SUMMARY}
          if ${{ inputs.test_type == 'single_cli' }}; then
            echo "Number of nodes in the cluster: ${{ inputs.node_number }}" >> ${GITHUB_STEP_SUMMARY}
          fi
          echo "Type of certificate for Rancher Manager: ${{ inputs.ca_type }}" >> ${GITHUB_STEP_SUMMARY}
          echo "Type of cluster deployed: ${CLUSTER_TYPE:-normal}" >> ${GITHUB_STEP_SUMMARY}
          echo "Bootstrap method: ${BOOTSTRAP_METHOD}" >> ${GITHUB_STEP_SUMMARY}
          echo "### Rancher Manager" >> ${GITHUB_STEP_SUMMARY}
          echo "Rancher Manager Image: ${{ steps.component.outputs.rm_version }}" >> ${GITHUB_STEP_SUMMARY}
          echo "Rancher Manager Version: ${{ inputs.rancher_version }}" >> ${GITHUB_STEP_SUMMARY}
          echo "CertManager Image: ${{ steps.component.outputs.cert_manager_version }}" >> ${GITHUB_STEP_SUMMARY}
          echo "### Elemental" >> ${GITHUB_STEP_SUMMARY}
          echo "Elemental ISO image: ${{ inputs.os_to_test }}" >> ${GITHUB_STEP_SUMMARY}
          echo "Elemental OS version: ${{ steps.iso_version.outputs.image_tag }}" >> ${GITHUB_STEP_SUMMARY}
          echo "Elemental Operator Image: ${{ steps.component.outputs.operator_version }}" >> ${GITHUB_STEP_SUMMARY}
          echo "Elemental Backup/Restore Operator Image: ${{ steps.component.outputs.backup_restore_version }}" >> ${GITHUB_STEP_SUMMARY}
          if ${{ inputs.elemental_ui_version != '' }}; then
            echo "Elemental UI Extension Version: ${{ inputs.elemental_ui_version }}" >> ${GITHUB_STEP_SUMMARY}
          fi
          if ${{ inputs.ui_account != '' }}; then
            echo "Elemental UI User: ${{ inputs.ui_account }}" >> ${GITHUB_STEP_SUMMARY}
          fi
          echo "### Kubernetes" >> ${GITHUB_STEP_SUMMARY}
          echo "K3s on Rancher Manager: ${{ env.INSTALL_K3S_VERSION }}" >> ${GITHUB_STEP_SUMMARY}
          echo "K8s version deployed on the cluster(s): ${{ inputs.k8s_version_to_provision }}" >> ${GITHUB_STEP_SUMMARY}
          echo "### Cluster nodes" >> ${GITHUB_STEP_SUMMARY}
          echo "Number of CPU: ${VCPU:-unknown}" >> ${GITHUB_STEP_SUMMARY}
          echo "Memory size: ${VMEM:-unknown}GB" >> ${GITHUB_STEP_SUMMARY}
          # Upgrade details
          if ${{ inputs.upgrade_image != '' }} || ${{ inputs.upgrade_os_channel != '' }}; then
            echo "## Upgrade details" >> ${GITHUB_STEP_SUMMARY}
            echo "Elemental Operator Upgrade: ${{ steps.operator_upgrade.outputs.operator_upgrade }}" >> ${GITHUB_STEP_SUMMARY}
            echo "Elemental Operator Image: ${{ steps.operator_upgrade.outputs.operator_version }}" >> ${GITHUB_STEP_SUMMARY}
            echo "Rancher Manager Image: ${{ steps.rancher_upgrade.outputs.rm_version }}" >> ${GITHUB_STEP_SUMMARY}
            echo "Rancher Manager Version: ${{ inputs.rancher_upgrade }}" >> ${GITHUB_STEP_SUMMARY}
            echo "Channel: ${{ inputs.upgrade_os_channel }}" >> ${GITHUB_STEP_SUMMARY}
            echo "Upgrade image: ${{ inputs.upgrade_image }}" >> ${GITHUB_STEP_SUMMARY}
          fi
      - name: Finalize Qase Run and publish Results
        id: finalize_qase
        if: ${{ always() && job.status != 'cancelled' }}
        run: |
          if [[ -n "${QASE_RUN_ID}" ]]; then
            cd tests && make publish-qase-run
          fi
      - name: Send failed status to slack
        id: send_status
        if: ${{ failure() && github.event_name == 'schedule' }}
        uses: slackapi/slack-github-action@v1.23.0
        with:
          payload: |
            {
              "blocks": [
                {
                  "type": "section",
                    "text": {
                      "type": "mrkdwn",
                      "text": "Workflow E2E ${{ github.job }}"
                    },
                    "accessory": {
                      "type": "button",
                      "text": {
                        "type": "plain_text",
                        "text": ":github:",
                         "emoji": true
                        },
                      "url": "${{ github.server_url }}/${{ github.repository }}/actions/runs/${{ github.run_id }}"
                    }
                  }
              ]
            }
        env:
          SLACK_WEBHOOK_URL: ${{ secrets.slack_webhook_url }}
          SLACK_WEBHOOK_TYPE: INCOMING_WEBHOOK
  clean-runner:
    if: ${{ always() }}
    needs: [create-runner, e2e]
    runs-on: ubuntu-latest
    env:
      # QASE variables
      QASE_API_TOKEN: ${{ secrets.qase_api_token }}
      QASE_PROJECT_CODE: ELEMENTAL
      QASE_RUN_ID: ${{ needs.e2e.outputs.qase_run_id }}
    steps:
      # actions/checkout MUST come before auth
      - name: Checkout
        uses: actions/checkout@v3
      - name: Authenticate to GCP
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.credentials }}
      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v1
      - name: Delete GCP secrets
        run: |
          gcloud --quiet secrets delete PAT_TOKEN_${{ needs.create-runner.outputs.uuid }} || true
          gcloud --quiet secrets delete GH_REPO_${{ needs.create-runner.outputs.uuid }} || true
      - name: Delete Qase Run if job has been cancelled
        if: ${{ always() && contains(needs.e2e.outputs.steps_status, 'cancelled') }}
        run: |
          if [[ -n "${QASE_RUN_ID}" ]]; then
            cd tests && make delete-qase-run
          fi
  delete-runner:
    if: ${{ always() && needs.create-runner.result == 'success' && inputs.destroy_runner == true }}
    needs: [create-runner, clean-runner]
    runs-on: ubuntu-latest
    steps:
      # actions/checkout MUST come before auth
      - name: Checkout
        uses: actions/checkout@v3
      - name: Authenticate to GCP
        uses: google-github-actions/auth@v1
        with:
          credentials_json: ${{ secrets.credentials }}
      - name: Setup gcloud
        uses: google-github-actions/setup-gcloud@v1
      - name: Delete runner
        run: |
          gcloud --quiet compute instances delete ${{ needs.create-runner.outputs.runner }} \
            --delete-disks all \
            --zone ${{ inputs.zone }}
