{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Overview \u00b6 Elemental is a software stack enabling a centralized, full cloud-native OS management with Kubernetes. Cluster Node OSes are built and maintained via container images through the and installed on new hosts using the . The and the enable Rancher Manager to fully control Elemental clusters, from the installation and management of the OS on the Nodes to the provisioning of new K3s or RKE2 clusters in a centralized way. Ready to give it a try? Get an Elemental Cluster up and running following the Quickstart section. Want more details? Take a look at the Architecture section or reach out to the Slack channel.","title":"Overview"},{"location":"#overview","text":"Elemental is a software stack enabling a centralized, full cloud-native OS management with Kubernetes. Cluster Node OSes are built and maintained via container images through the and installed on new hosts using the . The and the enable Rancher Manager to fully control Elemental clusters, from the installation and management of the OS on the Nodes to the provisioning of new K3s or RKE2 clusters in a centralized way. Ready to give it a try? Get an Elemental Cluster up and running following the Quickstart section. Want more details? Take a look at the Architecture section or reach out to the Slack channel.","title":"Overview"},{"location":"architecture/","text":"Architecture \u00b6 The Elemental stack can be divided in two main parts: the Elemental OS, an immutable and customizable OS which comprises the tools and the steps needed to prepare the Cloud Native OS image and perform the actual OS installation on the host, and the , that allows central management of the Elemental OS via Rancher, the Kubernetes way. Elemental OS \u00b6 In order to deploy the Elemental OS we need: an Elemental base OS image an Elemental installation configuration the tool, which installs the Elemental OS image on the target host applying the Elemental installation configuration Elemental OS image \u00b6 The Elemental OS image is an OCI container image containing all the files that will make up the OS of the target host. It will contain not only all the desired binaries and libraries, but also the kernel and the boot files required by a linux system. The is at the core of the Elemental OS, enabling to boot and upgrade an OS from container images. It also provides a framework that allows to combine different packages to bake custom OS container images. For more information check the project page. Elemental installation configuration \u00b6 In order to provision a machine with an Elemental OS image, installation configuration parameters are required: things such as the boot device, the root password, system configuration, users and custom files are things that should be provided aside from the Elemental OS image. All the data can be provided in a single .yaml file. More details can be found in the documentation. \u00b6 is the tool that allows to turn the Elemental OS image in a bootable and installed OS: it can generate an image from the provided Elemental OS container image. The generated image can be used to boot a virtual machine or a bare metal host and start the Elemental OS installation. The allows also to install the Elemental OS on the storage device of the live booted host, applying the provided Elemental installation configuration. For the list and syntax of the commands available in the , check the online documentation . \u00b6 The is a live ISO based on the Elemental OS (an Elemental live ISO). It includes all the tools needed to perform a full node provisioning, from the OS to Kubernetes, including the and the . \u00b6 The is responsible for managing OS upgrades and a secure device inventory to assist with zero touch provisioning. It provides an Helm Chart and an . Helm Chart \u00b6 The Helm Chart must be installed on a Rancher Cluster. It enables new hosts to: register against the . retrieve the Elemental installation configuration (which is stored in custom Kubernetes resources) to start the Elemental OS installation. download and install the , which enables Rancher to provision and manage K3s and RKE2 on the Elemental nodes. The allows control of the Elemental Nodes by extending the Kubernetes APIs with a set of elemental.cattle.io Kubernetes CRDs : MachineRegistration MachineInventory MachineInventorySelector MachineInventorySelectorTemplate ManagedOSImage ManagedOSVersion ManagedOSVersionChannel MachineRegistration \u00b6 The MachineRegistration includes the Elemental installation configuration (provided by the user) and a registration token (generated by the ), from which a registration URL is derived. The registration URL is the way through which an host can access the services, to kick off the Elemental provisioning process. The MachineRegistration has a Ready condition which turns to true when the has successfully generated the registration URL and an associated ServiceAccount. From this point on the target host can connect to the registration URL to kick off the provisioning process. An HTTP GET request against the registration URL returns the registration file : a .yaml file containing the registration data (i.e., the spec:config:elemental:registration section only from the just created MachineRegistration). The registration file contains all the required data to allow the target host to perform self registration and start the Elemental provisioning. See the section for more info on the registration process and the config:elemental:registration section in the MachineRegistration reference for more details on the available registration options. MachineInventory \u00b6 When a new host registers successfully, the creates a MachineInventory resource representing that particular host. The MachineInventory stores the TPM hash of the tracked host, retrieved during the registration process, and allows to execute arbitrary commands (plans) on the machine. A MachineInventory has three conditions: Initialized , tracking if the resources needed for applying the plan have been correctly created. PlanReady , showing if the host has completed its current plan. Ready , which indicates that a machine has been initialized and has no running plans. MachineInventorySelector \u00b6 A MachineInventorySelector selects MachineInventories based on applied selectors (usually patter matching on MachineInventory label values). MachineInventorySelectors have three conditions: InventoryReady , turn to true if the MachineInventorySelector has found a matching MachineInventory and has successful set itself as the MachineInventorySelector owner. BootstrapReady , reports if the selector has successfully applied its bootstrap plan. Ready , tracks if the inventory has been correctly selected and bootstrapped. MachineInventorySelectorTemplate \u00b6 The MachineInventorySelectorTemplate is a user defined resource that will be used as the blueprint to create the required MachineInventorySelectors: it includes the selector to identify the eligible MachineInventories. \u00b6 New hosts start the Elemental provisioning process through the : this tool requires a valid elemental-operator registration URL as input (see the MachineRegistration section ), and performs the following steps: setups a websocket connection to the registration URL authenticates itself using the registration token and the onboard TPM (Trusted Platform Module) sends SMBIOS data to the retrieves the Elemental installation configuration starts the and performs the Elemental OS installation Note if no TPM 2.0 is available on the host, TPM can be emulated by software: see the emulate-tpm key in the config.elemental.register reference document . includes a Kubernetes operator installed in the management cluster and a client side installed in nodes, so they can self register into the management cluster. Once a node is registered the will kick-start the OS installation and schedule the Kubernetes provisioning using the . Rancher System Agent is responsible for bootstrapping RKE2/k3s and Rancher from an OCI registry. This means an update of containerd, k3s, RKE2, or Rancher does not require an OS upgrade or node reboot. Elemental Teal \u00b6 Elemental Teal is Elemental OS built on top of SUSE Linux Enterprise (SLE) Micro for Rancher using the Elemental stack.","title":"Architecture"},{"location":"architecture/#architecture","text":"The Elemental stack can be divided in two main parts: the Elemental OS, an immutable and customizable OS which comprises the tools and the steps needed to prepare the Cloud Native OS image and perform the actual OS installation on the host, and the , that allows central management of the Elemental OS via Rancher, the Kubernetes way.","title":"Architecture"},{"location":"architecture/#elemental-os","text":"In order to deploy the Elemental OS we need: an Elemental base OS image an Elemental installation configuration the tool, which installs the Elemental OS image on the target host applying the Elemental installation configuration","title":"Elemental OS"},{"location":"architecture/#elemental-os-image","text":"The Elemental OS image is an OCI container image containing all the files that will make up the OS of the target host. It will contain not only all the desired binaries and libraries, but also the kernel and the boot files required by a linux system. The is at the core of the Elemental OS, enabling to boot and upgrade an OS from container images. It also provides a framework that allows to combine different packages to bake custom OS container images. For more information check the project page.","title":"Elemental OS image"},{"location":"architecture/#elemental-installation-configuration","text":"In order to provision a machine with an Elemental OS image, installation configuration parameters are required: things such as the boot device, the root password, system configuration, users and custom files are things that should be provided aside from the Elemental OS image. All the data can be provided in a single .yaml file. More details can be found in the documentation.","title":"Elemental installation configuration"},{"location":"architecture/#_1","text":"is the tool that allows to turn the Elemental OS image in a bootable and installed OS: it can generate an image from the provided Elemental OS container image. The generated image can be used to boot a virtual machine or a bare metal host and start the Elemental OS installation. The allows also to install the Elemental OS on the storage device of the live booted host, applying the provided Elemental installation configuration. For the list and syntax of the commands available in the , check the online documentation .","title":""},{"location":"architecture/#_2","text":"The is a live ISO based on the Elemental OS (an Elemental live ISO). It includes all the tools needed to perform a full node provisioning, from the OS to Kubernetes, including the and the .","title":""},{"location":"architecture/#_3","text":"The is responsible for managing OS upgrades and a secure device inventory to assist with zero touch provisioning. It provides an Helm Chart and an .","title":""},{"location":"architecture/#helm-chart","text":"The Helm Chart must be installed on a Rancher Cluster. It enables new hosts to: register against the . retrieve the Elemental installation configuration (which is stored in custom Kubernetes resources) to start the Elemental OS installation. download and install the , which enables Rancher to provision and manage K3s and RKE2 on the Elemental nodes. The allows control of the Elemental Nodes by extending the Kubernetes APIs with a set of elemental.cattle.io Kubernetes CRDs : MachineRegistration MachineInventory MachineInventorySelector MachineInventorySelectorTemplate ManagedOSImage ManagedOSVersion ManagedOSVersionChannel","title":" Helm Chart"},{"location":"architecture/#machineregistration","text":"The MachineRegistration includes the Elemental installation configuration (provided by the user) and a registration token (generated by the ), from which a registration URL is derived. The registration URL is the way through which an host can access the services, to kick off the Elemental provisioning process. The MachineRegistration has a Ready condition which turns to true when the has successfully generated the registration URL and an associated ServiceAccount. From this point on the target host can connect to the registration URL to kick off the provisioning process. An HTTP GET request against the registration URL returns the registration file : a .yaml file containing the registration data (i.e., the spec:config:elemental:registration section only from the just created MachineRegistration). The registration file contains all the required data to allow the target host to perform self registration and start the Elemental provisioning. See the section for more info on the registration process and the config:elemental:registration section in the MachineRegistration reference for more details on the available registration options.","title":"MachineRegistration"},{"location":"architecture/#machineinventory","text":"When a new host registers successfully, the creates a MachineInventory resource representing that particular host. The MachineInventory stores the TPM hash of the tracked host, retrieved during the registration process, and allows to execute arbitrary commands (plans) on the machine. A MachineInventory has three conditions: Initialized , tracking if the resources needed for applying the plan have been correctly created. PlanReady , showing if the host has completed its current plan. Ready , which indicates that a machine has been initialized and has no running plans.","title":"MachineInventory"},{"location":"architecture/#machineinventoryselector","text":"A MachineInventorySelector selects MachineInventories based on applied selectors (usually patter matching on MachineInventory label values). MachineInventorySelectors have three conditions: InventoryReady , turn to true if the MachineInventorySelector has found a matching MachineInventory and has successful set itself as the MachineInventorySelector owner. BootstrapReady , reports if the selector has successfully applied its bootstrap plan. Ready , tracks if the inventory has been correctly selected and bootstrapped.","title":"MachineInventorySelector"},{"location":"architecture/#machineinventoryselectortemplate","text":"The MachineInventorySelectorTemplate is a user defined resource that will be used as the blueprint to create the required MachineInventorySelectors: it includes the selector to identify the eligible MachineInventories.","title":"MachineInventorySelectorTemplate"},{"location":"architecture/#_4","text":"New hosts start the Elemental provisioning process through the : this tool requires a valid elemental-operator registration URL as input (see the MachineRegistration section ), and performs the following steps: setups a websocket connection to the registration URL authenticates itself using the registration token and the onboard TPM (Trusted Platform Module) sends SMBIOS data to the retrieves the Elemental installation configuration starts the and performs the Elemental OS installation Note if no TPM 2.0 is available on the host, TPM can be emulated by software: see the emulate-tpm key in the config.elemental.register reference document . includes a Kubernetes operator installed in the management cluster and a client side installed in nodes, so they can self register into the management cluster. Once a node is registered the will kick-start the OS installation and schedule the Kubernetes provisioning using the . Rancher System Agent is responsible for bootstrapping RKE2/k3s and Rancher from an OCI registry. This means an update of containerd, k3s, RKE2, or Rancher does not require an OS upgrade or node reboot.","title":""},{"location":"architecture/#elemental-teal","text":"Elemental Teal is Elemental OS built on top of SUSE Linux Enterprise (SLE) Micro for Rancher using the Elemental stack.","title":"Elemental Teal"},{"location":"backup/","text":"Backup \u00b6 Follow this guide to create backup for Elemental configuration installed together with Rancher. Install rancher-backup operator for Rancher \u00b6 Go to official Rancher documentation and install rancher-bakup operator from there. :::warning warning For Rancher v2.7 and below it is needed to edit ResourceSet for rancher-backup operator. For Rancher v2.7.1+ backup will be done automatically by rancher-backup operator and no further operation are needed. ::: Backup Elemental with rancher-backup operator (only for Rancher v2.7 and below) \u00b6 Fetch rancher-resource-set object from Kubernetes cluster ```shell showLineNumbers kubectl get ResourceSet rancher-resource-set -o yaml > rancher-resource-set.yaml <Tabs> <TabItem value=\"manualEdit\" label=\"Manually editing the resource set yaml\"> At the end of `rancher-resource-set.yaml` file add the definition of Elemental resources ```yaml showLineNumbers - apiVersion: apiextensions.k8s.io/v1 kindsRegexp: . resourceNameRegexp: elemental.cattle.io$ - apiVersion: apps/v1 kindsRegexp: ^deployments$ namespaces: - cattle-elemental-system resourceNames: - elemental-operator - apiVersion: rbac.authorization.k8s.io/v1 kindsRegexp: ^clusterroles$ resourceNames: - elemental-operator - apiVersion: rbac.authorization.k8s.io/v1 kindsRegexp: ^clusterrolebindings$ resourceNames: - elemental-operator - apiVersion: v1 kindsRegexp: ^serviceaccounts$ namespaces: - cattle-elemental-system resourceNames: - elemental-operator - apiVersion: management.cattle.io/v3 kindsRegexp: ^globalrole$ resourceNames: - elemental-operator - apiVersion: management.cattle.io/v3 kindsRegexp: ^apiservice$ resourceNameRegexp: elemental.cattle.io$ - apiVersion: elemental.cattle.io/v1beta1 kindsRegexp: . namespaceRegexp: ^cattle-fleet-|^fleet-|^cluster-fleet- - apiVersion: rbac.authorization.k8s.io/v1 kindsRegexp: ^roles$|^rolebindings$ labelSelectors: matchExpressions: - key: elemental.cattle.io/managed operator: In values: - \"true\" namespaceRegexp: ^cattle-fleet-|^fleet-|^cluster-fleet- - apiVersion: v1 kindsRegexp: ^secrets$|^serviceaccounts$ labelSelectors: matchExpressions: - key: elemental.cattle.io/managed operator: In values: - \"true\" namespaceRegexp: ^cattle-fleet-|^fleet-|^cluster-fleet- You can use yq to auto merge rancher-resource-set.yaml and elemental-resource-set.yaml . Please go and install yq v4.x version Create elemental-resource-set.yaml file ```yaml showLineNumbers apiVersion: resources.cattle.io/v1 kind: ResourceSet metadata: name: rancher-resource-set resourceSelectors: - apiVersion: apiextensions.k8s.io/v1 kindsRegexp: . resourceNameRegexp: elemental.cattle.io$ - apiVersion: apps/v1 kindsRegexp: ^deployments$ namespaces: - cattle-elemental-system resourceNames: - elemental-operator - apiVersion: rbac.authorization.k8s.io/v1 kindsRegexp: ^clusterroles$ resourceNames: - elemental-operator - apiVersion: rbac.authorization.k8s.io/v1 kindsRegexp: ^clusterrolebindings$ resourceNames: - elemental-operator - apiVersion: v1 kindsRegexp: ^serviceaccounts$ namespaces: - cattle-elemental-system resourceNames: - elemental-operator - apiVersion: management.cattle.io/v3 kindsRegexp: ^globalrole$ resourceNames: - elemental-operator - apiVersion: management.cattle.io/v3 kindsRegexp: ^apiservice$ resourceNameRegexp: elemental.cattle.io$ - apiVersion: elemental.cattle.io/v1beta1 kindsRegexp: . namespaceRegexp: ^cattle-fleet-|^fleet-|^cluster-fleet- - apiVersion: rbac.authorization.k8s.io/v1 kindsRegexp: ^roles$|^rolebindings$ labelSelectors: matchExpressions: - key: elemental.cattle.io/managed operator: In values: - \"true\" namespaceRegexp: ^cattle-fleet-|^fleet-|^cluster-fleet- - apiVersion: v1 kindsRegexp: ^secrets$|^serviceaccounts$ labelSelectors: matchExpressions: - key: elemental.cattle.io/managed operator: In values: - \"true\" namespaceRegexp: ^cattle-fleet-|^fleet-|^cluster-fleet- To merge both files, use `yq` command ```shell showLineNumbers yq ea --inplace '. as $item ireduce ({}; . *+ $item )' rancher-resource-set.yaml elemental-resource-set.yaml Then apply changes to Kubernetes cluster ```shell showLineNumbers kubectl apply -f rancher-resource-set.yaml Create backup with creating Backup object ```yaml showLineNumbers apiVersion: resources.cattle.io/v1 kind: Backup metadata: name: elemental-backup spec: resourceSetName: rancher-resource-set schedule: \"10 3 * * *\" retentionCount: 10 Check logs from rancher-backup operator ```shell showLineNumbers kubectl logs -n cattle-resources-system -l app.kubernetes.io/name=rancher-backup -f Verify if backup file was created on Persistent Volume. ```shell showLineNumbers ... INFO[2022/10/17 07:45:04] Finding files starting with /var/lib/backups/rancher-backup-430169aa-edde-4a61-85e8-858f625a755b*.tar.gz INFO[2022/10/17 07:45:04] File rancher-backup-430169aa-edde-4a61-85e8-858f625a755b-2022-10-17T05-15-00Z.tar.gz was created at 2022-10-17 0 ...","title":""},{"location":"backup/#backup","text":"Follow this guide to create backup for Elemental configuration installed together with Rancher.","title":"Backup"},{"location":"backup/#install-rancher-backup-operator-for-rancher","text":"Go to official Rancher documentation and install rancher-bakup operator from there. :::warning warning For Rancher v2.7 and below it is needed to edit ResourceSet for rancher-backup operator. For Rancher v2.7.1+ backup will be done automatically by rancher-backup operator and no further operation are needed. :::","title":"Install rancher-backup operator for Rancher"},{"location":"backup/#backup-elemental-with-rancher-backup-operator-only-for-rancher-v27-and-below","text":"Fetch rancher-resource-set object from Kubernetes cluster ```shell showLineNumbers kubectl get ResourceSet rancher-resource-set -o yaml > rancher-resource-set.yaml <Tabs> <TabItem value=\"manualEdit\" label=\"Manually editing the resource set yaml\"> At the end of `rancher-resource-set.yaml` file add the definition of Elemental resources ```yaml showLineNumbers - apiVersion: apiextensions.k8s.io/v1 kindsRegexp: . resourceNameRegexp: elemental.cattle.io$ - apiVersion: apps/v1 kindsRegexp: ^deployments$ namespaces: - cattle-elemental-system resourceNames: - elemental-operator - apiVersion: rbac.authorization.k8s.io/v1 kindsRegexp: ^clusterroles$ resourceNames: - elemental-operator - apiVersion: rbac.authorization.k8s.io/v1 kindsRegexp: ^clusterrolebindings$ resourceNames: - elemental-operator - apiVersion: v1 kindsRegexp: ^serviceaccounts$ namespaces: - cattle-elemental-system resourceNames: - elemental-operator - apiVersion: management.cattle.io/v3 kindsRegexp: ^globalrole$ resourceNames: - elemental-operator - apiVersion: management.cattle.io/v3 kindsRegexp: ^apiservice$ resourceNameRegexp: elemental.cattle.io$ - apiVersion: elemental.cattle.io/v1beta1 kindsRegexp: . namespaceRegexp: ^cattle-fleet-|^fleet-|^cluster-fleet- - apiVersion: rbac.authorization.k8s.io/v1 kindsRegexp: ^roles$|^rolebindings$ labelSelectors: matchExpressions: - key: elemental.cattle.io/managed operator: In values: - \"true\" namespaceRegexp: ^cattle-fleet-|^fleet-|^cluster-fleet- - apiVersion: v1 kindsRegexp: ^secrets$|^serviceaccounts$ labelSelectors: matchExpressions: - key: elemental.cattle.io/managed operator: In values: - \"true\" namespaceRegexp: ^cattle-fleet-|^fleet-|^cluster-fleet- You can use yq to auto merge rancher-resource-set.yaml and elemental-resource-set.yaml . Please go and install yq v4.x version Create elemental-resource-set.yaml file ```yaml showLineNumbers apiVersion: resources.cattle.io/v1 kind: ResourceSet metadata: name: rancher-resource-set resourceSelectors: - apiVersion: apiextensions.k8s.io/v1 kindsRegexp: . resourceNameRegexp: elemental.cattle.io$ - apiVersion: apps/v1 kindsRegexp: ^deployments$ namespaces: - cattle-elemental-system resourceNames: - elemental-operator - apiVersion: rbac.authorization.k8s.io/v1 kindsRegexp: ^clusterroles$ resourceNames: - elemental-operator - apiVersion: rbac.authorization.k8s.io/v1 kindsRegexp: ^clusterrolebindings$ resourceNames: - elemental-operator - apiVersion: v1 kindsRegexp: ^serviceaccounts$ namespaces: - cattle-elemental-system resourceNames: - elemental-operator - apiVersion: management.cattle.io/v3 kindsRegexp: ^globalrole$ resourceNames: - elemental-operator - apiVersion: management.cattle.io/v3 kindsRegexp: ^apiservice$ resourceNameRegexp: elemental.cattle.io$ - apiVersion: elemental.cattle.io/v1beta1 kindsRegexp: . namespaceRegexp: ^cattle-fleet-|^fleet-|^cluster-fleet- - apiVersion: rbac.authorization.k8s.io/v1 kindsRegexp: ^roles$|^rolebindings$ labelSelectors: matchExpressions: - key: elemental.cattle.io/managed operator: In values: - \"true\" namespaceRegexp: ^cattle-fleet-|^fleet-|^cluster-fleet- - apiVersion: v1 kindsRegexp: ^secrets$|^serviceaccounts$ labelSelectors: matchExpressions: - key: elemental.cattle.io/managed operator: In values: - \"true\" namespaceRegexp: ^cattle-fleet-|^fleet-|^cluster-fleet- To merge both files, use `yq` command ```shell showLineNumbers yq ea --inplace '. as $item ireduce ({}; . *+ $item )' rancher-resource-set.yaml elemental-resource-set.yaml Then apply changes to Kubernetes cluster ```shell showLineNumbers kubectl apply -f rancher-resource-set.yaml Create backup with creating Backup object ```yaml showLineNumbers apiVersion: resources.cattle.io/v1 kind: Backup metadata: name: elemental-backup spec: resourceSetName: rancher-resource-set schedule: \"10 3 * * *\" retentionCount: 10 Check logs from rancher-backup operator ```shell showLineNumbers kubectl logs -n cattle-resources-system -l app.kubernetes.io/name=rancher-backup -f Verify if backup file was created on Persistent Volume. ```shell showLineNumbers ... INFO[2022/10/17 07:45:04] Finding files starting with /var/lib/backups/rancher-backup-430169aa-edde-4a61-85e8-858f625a755b*.tar.gz INFO[2022/10/17 07:45:04] File rancher-backup-430169aa-edde-4a61-85e8-858f625a755b-2022-10-17T05-15-00Z.tar.gz was created at 2022-10-17 0 ...","title":"Backup Elemental with rancher-backup operator (only for Rancher v2.7 and below)"},{"location":"cloud-config-reference/","text":"Cloud-config Reference \u00b6 All custom configuration applied on top of a fresh deployment should come from the cloud-config section in a MachineRegistration . This will get run by elemental-cli run-stage during the boot stage, and it will be stored in the node under the /oem dir. Elemental uses yip to run these cloud-config files, so we support the yip subset cloud-config implementation . Below is an example of the supported configuration on a MachineRegistration resource. Example ```yaml showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: config: cloud-config: users: - name: \"bar\" passwd: \"foo\" groups: \"users\" homedir: \"/home/foo\" shell: \"/bin/bash\" ssh_authorized_keys: - faaapploo # Assigns these keys to the first user in users or root if there # is none ssh_authorized_keys: - asdd # Run these commands once the system has fully booted runcmd: - foo # Write arbitrary files write_files: - encoding: b64 content: CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4 path: /foo/bar permissions: \"0644\" owner: \"bar\" elemental: install: reboot: true device: /dev/sda debug: true machineName: my-machine machineInventoryLabels: location: \"europe\" ```","title":""},{"location":"cloud-config-reference/#cloud-config-reference","text":"All custom configuration applied on top of a fresh deployment should come from the cloud-config section in a MachineRegistration . This will get run by elemental-cli run-stage during the boot stage, and it will be stored in the node under the /oem dir. Elemental uses yip to run these cloud-config files, so we support the yip subset cloud-config implementation . Below is an example of the supported configuration on a MachineRegistration resource. Example ```yaml showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: config: cloud-config: users: - name: \"bar\" passwd: \"foo\" groups: \"users\" homedir: \"/home/foo\" shell: \"/bin/bash\" ssh_authorized_keys: - faaapploo # Assigns these keys to the first user in users or root if there # is none ssh_authorized_keys: - asdd # Run these commands once the system has fully booted runcmd: - foo # Write arbitrary files write_files: - encoding: b64 content: CiMgVGhpcyBmaWxlIGNvbnRyb2xzIHRoZSBzdGF0ZSBvZiBTRUxpbnV4 path: /foo/bar permissions: \"0644\" owner: \"bar\" elemental: install: reboot: true device: /dev/sda debug: true machineName: my-machine machineInventoryLabels: location: \"europe\" ```","title":"Cloud-config Reference"},{"location":"cluster-reference/","text":"import Machinepools from \"!!raw-loader!../examples/clusters/clusters-several-machinepools.yml\" Cluster reference \u00b6 A Cluster definition includes a kubernetesVersion and a list of machinePools to deploy the cluster to. For how to select a kubernetesVersion please check our Kubernetes Versions page. A machinePool is a bundle of configuration with a ObjectReference so the cluster is deployed to those machinePools with the proper roles (etcd, control-plane, worker) with a quantity (how many nodes to deploy from this pool) and some extra configurations (rolling update config, max unhelthy nodes, etc...). Example ```yaml showLineNumbers kind: Cluster apiVersion: provisioning.cattle.io/v1 metadata: name: ... namespace: ... spec: rkeConfig: machinePools: - name: ... controlPlaneRole: ... etcdRole: ... workerRole: ... quantity: ... machineConfigRef: apiVersion: elemental.cattle.io/v1beta1 kind: MachineInventorySelectorTemplate name: ... - name: ... controlPlaneRole: ... etcdRole: ... workerRole: ... quantity: ... machineConfigRef: apiVersion: elemental.cattle.io/v1beta1 kind: MachineInventorySelectorTemplate name: ... ``` rkeConfig.machinePools \u00b6 A list of machinePools . A minimun of 1 machinePools is required for the cluster to be deployed to. machinePools Spec Reference \u00b6 Key Type Default value Description controlPlaneRole bool false Set machines in this pool as control-plane etcdRole bool false Set machines in this pool as etcd workerRole bool false Set machines in this pool as worker name string nil Name for this pool quantity int nil Number of machines to deploy from this pool unhealthyNodeTimeout int nil Timeout for unhealthy node health checks machineConfigRef int ObjectReference Reference to an object used to know what nodes are part of this pool A minimum of quantity set to one is required for this pool to be used. Basically translates to how many nodes from this pool are going to be setup for this cluster. Example ```yaml showLineNumbers kind: Cluster apiVersion: provisioning.cattle.io/v1 metadata: name: cluster-example namespace: example-default spec: rkeConfig: machinePools: - name: examplePool controlPlaneRole: true etcdRole: true workerRole: false quantity: 3 unhealthyNodeTimeout: 0s machineConfigRef: apiVersion: elemental.cattle.io/v1beta1 kind: MachineInventorySelectorTemplate name: exampleSelector ``` machineConfigRef Spec Reference \u00b6 A machineConfigRef is a generic k8s ObjectReference which usually contain a kind name and apiVersion to point to a different object. In Elemental, we set this to a MachineInventorySelectorTemplate . This allows us to point to more than one object by using the selector. Example \u00b6 The example below creates a cluster that uses 2 different machinePool s to set different nodes to control-plane and workers nodes, based on 2 different MachineInventorySelectorTemplate that select their nodes based on a MachineInventory label (location) :::warning warning The labels for the example are manual set labels, they are not set by Elemental automatically.. For automatic labels generated by Elemental please check the SMBIOS page. ::: {Machinepools}","title":""},{"location":"cluster-reference/#cluster-reference","text":"A Cluster definition includes a kubernetesVersion and a list of machinePools to deploy the cluster to. For how to select a kubernetesVersion please check our Kubernetes Versions page. A machinePool is a bundle of configuration with a ObjectReference so the cluster is deployed to those machinePools with the proper roles (etcd, control-plane, worker) with a quantity (how many nodes to deploy from this pool) and some extra configurations (rolling update config, max unhelthy nodes, etc...). Example ```yaml showLineNumbers kind: Cluster apiVersion: provisioning.cattle.io/v1 metadata: name: ... namespace: ... spec: rkeConfig: machinePools: - name: ... controlPlaneRole: ... etcdRole: ... workerRole: ... quantity: ... machineConfigRef: apiVersion: elemental.cattle.io/v1beta1 kind: MachineInventorySelectorTemplate name: ... - name: ... controlPlaneRole: ... etcdRole: ... workerRole: ... quantity: ... machineConfigRef: apiVersion: elemental.cattle.io/v1beta1 kind: MachineInventorySelectorTemplate name: ... ```","title":"Cluster reference"},{"location":"cluster-reference/#rkeconfigmachinepools","text":"A list of machinePools . A minimun of 1 machinePools is required for the cluster to be deployed to.","title":"rkeConfig.machinePools"},{"location":"cluster-reference/#machinepools-spec-reference","text":"Key Type Default value Description controlPlaneRole bool false Set machines in this pool as control-plane etcdRole bool false Set machines in this pool as etcd workerRole bool false Set machines in this pool as worker name string nil Name for this pool quantity int nil Number of machines to deploy from this pool unhealthyNodeTimeout int nil Timeout for unhealthy node health checks machineConfigRef int ObjectReference Reference to an object used to know what nodes are part of this pool A minimum of quantity set to one is required for this pool to be used. Basically translates to how many nodes from this pool are going to be setup for this cluster. Example ```yaml showLineNumbers kind: Cluster apiVersion: provisioning.cattle.io/v1 metadata: name: cluster-example namespace: example-default spec: rkeConfig: machinePools: - name: examplePool controlPlaneRole: true etcdRole: true workerRole: false quantity: 3 unhealthyNodeTimeout: 0s machineConfigRef: apiVersion: elemental.cattle.io/v1beta1 kind: MachineInventorySelectorTemplate name: exampleSelector ```","title":"machinePools Spec Reference"},{"location":"cluster-reference/#machineconfigref-spec-reference","text":"A machineConfigRef is a generic k8s ObjectReference which usually contain a kind name and apiVersion to point to a different object. In Elemental, we set this to a MachineInventorySelectorTemplate . This allows us to point to more than one object by using the selector.","title":"machineConfigRef Spec Reference"},{"location":"cluster-reference/#example","text":"The example below creates a cluster that uses 2 different machinePool s to set different nodes to control-plane and workers nodes, based on 2 different MachineInventorySelectorTemplate that select their nodes based on a MachineInventory label (location) :::warning warning The labels for the example are manual set labels, they are not set by Elemental automatically.. For automatic labels generated by Elemental please check the SMBIOS page. ::: {Machinepools}","title":"Example"},{"location":"customizing/","text":"Custom Images \u00b6 Elemental Teal images can be customized in different ways. One option is to provide additional resources within the installation media so that during installation, or eventually at boot time, additional binaries such as drivers can be included. Another option would be to remaster the Elemental Teal by simply using a docker build. Elemental Teal is a regular container image, so it is absolutely possible to create a new image using a Dockerfile based on Elemental Teal image. Customize installation ISO and installation process \u00b6 In order to adapt the installation ISO a simple approach is to append extra configuration files into the ISO root in the same way a registration yaml configuration file is added. Additional configuration files \u00b6 Elemental Teal installation can be customized in three different, non-exclusive ways. First, including some custom Elemental client configuration file, second, by including additional cloud-init files to execute at boot time, and finally, by including installation hooks. Custom Elemental client configuration file \u00b6 Elemental client install , upgrade and reset commands can be configured with a custom configuration file . In order to set a custom configuration file in the installation media the MachineRegistration resource associated with this ISO should also include the Elemental client configuration directory. For that purpose, the install field supports the config-dir field. See MachineRegistration reference and the example below: ```yaml showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: ... config: ... elemental: ... install: ... config-dir: \"/run/initramfs/live/elemental.conf.d\" Elemental Teal live ISOs, when booted, have the ISO root mounted at `/run/initramfs/live`. So in that case, the ISO will contain the custom Elemental client configuration file as `/elemental.conf.d/config.yaml`. #### Adding additional cloud-init files at boot In order to include additional cloud-init files during the installation they need to be added to the installation data into the MachineRegistration resource. More specific the `config-urls` field is used for this exact purpose. See [MachineRegistration reference](../machineregistration-reference) page. `config-urls` is a list of string literals where each item is an http url pointing to a cloud-init file or a local path of a cloud init file. Note the local path is evaluated at the time of execution by the installation media, hence the local path must exist within the installation media, commonly an ISO image. Since in Elemental Teal live systems the ISO root is mounted at `/run/initramfs/live`, the local paths for `config-url` in MachineRegistrations are likely to point there. See the example below: ```yaml showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: ... config: ... elemental: ... install: ... config-urls: - \"/run/initramfs/live/oem/10_install_extra_drivers.yaml\" In that case the ISO root is expected to include the /oem/10_install_extra_drivers.yaml file. Installation hooks \u00b6 Elemental client install , upgrade and reset procedures include three different hooks: before-install : executed after all partition mountpoints are set. after-install-chroot : executed after deploying the OS image and before unmounting the associated loop filesystem image. Runs chrooted to the OS image. after-install : executed before unmounting partitions but after all OS images are set and unmounted. Hooks are provided as cloud-init stages. Equivalent hooks exist for reset and upgrade procedures. Hooks are evaluated at install , reset and upgrade processes from /oem , /system/oem and /usr/local/cloud-config , however additional paths can be provided with the cloud-init-paths flag in Elemental client configuration . Adding extra driver binaries into the ISO example \u00b6 This example is covering the case in which extra driver binaries are included into the ISO and during the installation they are installed over the OS image. For that use case the following files are required: additional binaries to install (they could be in the form of RPMs) additional hooks file to copy binaries into the persistent storage and to install them additional Elemental client configuration file to point hooks file location Lets create an overlay directory to include the overlay root-tree that needs to be applied over the ISO root. In that case the overlay directory could contain: ```yaml showLineNumbers overlay/ data/ extra_drivers/ some_driver.rpm hooks/ install_hooks.yaml elemental/ config.yaml The Elemental client config file in `overlay/elemental` could be as: ```yaml showLineNumbers cloud-init-paths: - \"/run/initramfs/live/hooks\" This is just to let Elemental client know where to find installation hooks. Finally, the overlay/hooks/install_hooks.yaml could be as: ``yaml showLineNumbers name: \"Install extra drivers\" stages: before-install: # Preload data to the persistent storage # During installation persistent partition is mounted at /run/cos/persistent - commands: - rsync -a /run/initramfs/live/data/ /run/cos/persistent after-install-chroot: # extra_drivers folder is at /usr/local/extra_drivers` from the OS image chroot - commands: - rpm -iv /usr/local/extra_drivers/some_driver.rpm Note the installation hooks only cover installation procedures, for upgrades equivalent `before-upgrade` and/or `after-upgrade-chroot` should be defined. ### Repacking the ISO image with extra files Assuming an `overlay` folder was created in the current directory containing all additional files to be appended, the following `xorriso` command adds the extra files: ```bash showLineNumbers xorriso -indev elemental-teal.x86_64.iso -outdev elemental-teal.custom.x86_64.iso -map overlay / -boot_image any replay For that a xorriso equal or higher than version 1.5 is required. Remastering a custom docker image \u00b6 Since Elemental Teal image is a Docker image it can also be used as a base image in a Dockerfile in order to create a new container image. Imagine some additional package from an extra repository is required, the following example show case how this could be added: ```docker showLineNumbers The version of Elemental to modify \u00b6 FROM registry.opensuse.org/isv/rancher/elemental/teal52/15.3/rancher/elemental-node-image/5.2:VERSION Custom commands \u00b6 RUN rpm --import && \\ zypper addrepo --refresh extra_repo && \\ zypper install -y IMPORTANT: /etc/os-release is used for versioning/upgrade. The \u00b6 values here should reflect the tag of the image currently being built \u00b6 ARG IMAGE_REPO=norepo ARG IMAGE_TAG=latest RUN echo \"IMAGE_REPO=${IMAGE_REPO}\" > /etc/os-release && \\ echo \"IMAGE_TAG=${IMAGE_TAG}\" >> /etc/os-release && \\ echo \"IMAGE=${IMAGE_REPO}:${IMAGE_TAG}\" >> /etc/os-release Where VERSION is the base version we want to customize. And then the following commands ```bash showLineNumbers docker build --build-arg IMAGE_REPO=myrepo/custom-build \\ --build-arg IMAGE_TAG=v1.1.1 \\ -t myrepo/custom-build:v1.1.1 . docker push myrepo/custom-build:v1.1.1 The new customized OS is available as the Docker image myrepo/custom-build:v1.1.1 and it can be run and verified using docker with bash showLineNumbers docker run -it myrepo/custom-build:v1.1.1 bash","title":""},{"location":"customizing/#custom-images","text":"Elemental Teal images can be customized in different ways. One option is to provide additional resources within the installation media so that during installation, or eventually at boot time, additional binaries such as drivers can be included. Another option would be to remaster the Elemental Teal by simply using a docker build. Elemental Teal is a regular container image, so it is absolutely possible to create a new image using a Dockerfile based on Elemental Teal image.","title":"Custom Images"},{"location":"customizing/#customize-installation-iso-and-installation-process","text":"In order to adapt the installation ISO a simple approach is to append extra configuration files into the ISO root in the same way a registration yaml configuration file is added.","title":"Customize installation ISO and installation process"},{"location":"customizing/#additional-configuration-files","text":"Elemental Teal installation can be customized in three different, non-exclusive ways. First, including some custom Elemental client configuration file, second, by including additional cloud-init files to execute at boot time, and finally, by including installation hooks.","title":"Additional configuration files"},{"location":"customizing/#custom-elemental-client-configuration-file","text":"Elemental client install , upgrade and reset commands can be configured with a custom configuration file . In order to set a custom configuration file in the installation media the MachineRegistration resource associated with this ISO should also include the Elemental client configuration directory. For that purpose, the install field supports the config-dir field. See MachineRegistration reference and the example below: ```yaml showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: ... config: ... elemental: ... install: ... config-dir: \"/run/initramfs/live/elemental.conf.d\" Elemental Teal live ISOs, when booted, have the ISO root mounted at `/run/initramfs/live`. So in that case, the ISO will contain the custom Elemental client configuration file as `/elemental.conf.d/config.yaml`. #### Adding additional cloud-init files at boot In order to include additional cloud-init files during the installation they need to be added to the installation data into the MachineRegistration resource. More specific the `config-urls` field is used for this exact purpose. See [MachineRegistration reference](../machineregistration-reference) page. `config-urls` is a list of string literals where each item is an http url pointing to a cloud-init file or a local path of a cloud init file. Note the local path is evaluated at the time of execution by the installation media, hence the local path must exist within the installation media, commonly an ISO image. Since in Elemental Teal live systems the ISO root is mounted at `/run/initramfs/live`, the local paths for `config-url` in MachineRegistrations are likely to point there. See the example below: ```yaml showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: ... config: ... elemental: ... install: ... config-urls: - \"/run/initramfs/live/oem/10_install_extra_drivers.yaml\" In that case the ISO root is expected to include the /oem/10_install_extra_drivers.yaml file.","title":"Custom Elemental client configuration file"},{"location":"customizing/#installation-hooks","text":"Elemental client install , upgrade and reset procedures include three different hooks: before-install : executed after all partition mountpoints are set. after-install-chroot : executed after deploying the OS image and before unmounting the associated loop filesystem image. Runs chrooted to the OS image. after-install : executed before unmounting partitions but after all OS images are set and unmounted. Hooks are provided as cloud-init stages. Equivalent hooks exist for reset and upgrade procedures. Hooks are evaluated at install , reset and upgrade processes from /oem , /system/oem and /usr/local/cloud-config , however additional paths can be provided with the cloud-init-paths flag in Elemental client configuration .","title":"Installation hooks"},{"location":"customizing/#adding-extra-driver-binaries-into-the-iso-example","text":"This example is covering the case in which extra driver binaries are included into the ISO and during the installation they are installed over the OS image. For that use case the following files are required: additional binaries to install (they could be in the form of RPMs) additional hooks file to copy binaries into the persistent storage and to install them additional Elemental client configuration file to point hooks file location Lets create an overlay directory to include the overlay root-tree that needs to be applied over the ISO root. In that case the overlay directory could contain: ```yaml showLineNumbers overlay/ data/ extra_drivers/ some_driver.rpm hooks/ install_hooks.yaml elemental/ config.yaml The Elemental client config file in `overlay/elemental` could be as: ```yaml showLineNumbers cloud-init-paths: - \"/run/initramfs/live/hooks\" This is just to let Elemental client know where to find installation hooks. Finally, the overlay/hooks/install_hooks.yaml could be as: ``yaml showLineNumbers name: \"Install extra drivers\" stages: before-install: # Preload data to the persistent storage # During installation persistent partition is mounted at /run/cos/persistent - commands: - rsync -a /run/initramfs/live/data/ /run/cos/persistent after-install-chroot: # extra_drivers folder is at /usr/local/extra_drivers` from the OS image chroot - commands: - rpm -iv /usr/local/extra_drivers/some_driver.rpm Note the installation hooks only cover installation procedures, for upgrades equivalent `before-upgrade` and/or `after-upgrade-chroot` should be defined. ### Repacking the ISO image with extra files Assuming an `overlay` folder was created in the current directory containing all additional files to be appended, the following `xorriso` command adds the extra files: ```bash showLineNumbers xorriso -indev elemental-teal.x86_64.iso -outdev elemental-teal.custom.x86_64.iso -map overlay / -boot_image any replay For that a xorriso equal or higher than version 1.5 is required.","title":"Adding extra driver binaries into the ISO example"},{"location":"customizing/#remastering-a-custom-docker-image","text":"Since Elemental Teal image is a Docker image it can also be used as a base image in a Dockerfile in order to create a new container image. Imagine some additional package from an extra repository is required, the following example show case how this could be added: ```docker showLineNumbers","title":"Remastering a custom docker image"},{"location":"customizing/#the-version-of-elemental-to-modify","text":"FROM registry.opensuse.org/isv/rancher/elemental/teal52/15.3/rancher/elemental-node-image/5.2:VERSION","title":"The version of Elemental to modify"},{"location":"customizing/#custom-commands","text":"RUN rpm --import && \\ zypper addrepo --refresh extra_repo && \\ zypper install -y","title":"Custom commands"},{"location":"customizing/#important-etcos-release-is-used-for-versioningupgrade-the","text":"","title":"IMPORTANT: /etc/os-release is used for versioning/upgrade. The"},{"location":"customizing/#values-here-should-reflect-the-tag-of-the-image-currently-being-built","text":"ARG IMAGE_REPO=norepo ARG IMAGE_TAG=latest RUN echo \"IMAGE_REPO=${IMAGE_REPO}\" > /etc/os-release && \\ echo \"IMAGE_TAG=${IMAGE_TAG}\" >> /etc/os-release && \\ echo \"IMAGE=${IMAGE_REPO}:${IMAGE_TAG}\" >> /etc/os-release Where VERSION is the base version we want to customize. And then the following commands ```bash showLineNumbers docker build --build-arg IMAGE_REPO=myrepo/custom-build \\ --build-arg IMAGE_TAG=v1.1.1 \\ -t myrepo/custom-build:v1.1.1 . docker push myrepo/custom-build:v1.1.1 The new customized OS is available as the Docker image myrepo/custom-build:v1.1.1 and it can be run and verified using docker with bash showLineNumbers docker run -it myrepo/custom-build:v1.1.1 bash","title":"values here should reflect the tag of the image currently being built"},{"location":"elemental-plans/","text":"Introduction \u00b6 Elemental uses the Rancher System Agent , renamed to Elemental System Agent, to initially bootstrap the node with a simple plan. The plan will apply the following configurations: Set some labels for the node Set the proper hostname according to the MachineInventory value Install the default Rancher System Agent from Rancher Server, and install the proper Kubernetes components The bootstrap service also accepts local plans stored under /var/lib/elemental/agent/plans . Any plan written in there will also be applied during the initial node start after the installation is completed. :::tip The local plans run only during the initial Elemental bootstrap before Kubernetes is installed on the node. ::: Types of Plans \u00b6 The type of plans that Elemental can use are: One time instructions: Only run once Periodic instructions: They run periodically Files: Creates files Probes: http probes :::tip Both one time instructions and periodic instructions can run either a direct command or a docker image. ::: Adding local plans on Elemental \u00b6 You can add local plans to Elemental as part of the MachineRegistration CRD, in the cloud-config section as follows: ```yaml showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: config: cloud-config: users: - name: root passwd: root write_files: - path: /var/lib/elemental/agent/plans/mycustomplan.plan permissions: \"0600\" content: | {\"instructions\": [ { \"name\":\"set hostname\", \"command\":\"hostnamectl\", \"args\": [\"set-hostname\", \"myHostname\"] }, { \"name\":\"stop sshd service\", \"command\":\"systemctl\", \"args\": [\"stop\", \"sshd\"] } ] } elemental: install: reboot: true device: /dev/sda debug: true machineName: my-machine machineInventoryLabels: location: \"europe\" ## Plan examples The following plans are provided as a quick reference and are not guaranteed to work in your environment. To learn more about plans please check [Rancher System Agent](https://github.com/rancher/system-agent). <Tabs> <TabItem value=\"example1\" label=\"Example 1: one time instructions\" default> ```json showLineNumbers {\"instructions\": [ { \"name\":\"set hostname\", \"command\":\"hostnamectl\", \"args\": [\"set-hostname\", \"myHostname\"] }, { \"name\":\"stop sshd service\", \"command\":\"systemctl\", \"args\": [\"stop\", \"sshd\"] } ] } ```json showLineNumbers {\"periodicInstructions\": [ { \"name\":\"set hostname\", \"image\":\"ghcr.io/rancher-sandbox/elemental-example-plan:main\" \"command\": \"run.sh\" } ] } </TabItem> <TabItem value=\"example3\" label=\"Example 3: files\"> ```json showLineNumbers {\"files\": [ { \"content\":\"Welcome to the system\", \"path\":\"/etc/motd\", \"permissions\": \"0644\" } ] } json showLineNumbers {\"probes\": \"probe1\": { \"name\": \"Service Up\", \"httpGet\": { \"url\": \"http://10.0.0.1/healthz\", \"insecure\": \"false\", \"clientCert\": \"....\", \"clientKey\": \"....\", \"caCert\": \".....\" } } }","title":""},{"location":"elemental-plans/#introduction","text":"Elemental uses the Rancher System Agent , renamed to Elemental System Agent, to initially bootstrap the node with a simple plan. The plan will apply the following configurations: Set some labels for the node Set the proper hostname according to the MachineInventory value Install the default Rancher System Agent from Rancher Server, and install the proper Kubernetes components The bootstrap service also accepts local plans stored under /var/lib/elemental/agent/plans . Any plan written in there will also be applied during the initial node start after the installation is completed. :::tip The local plans run only during the initial Elemental bootstrap before Kubernetes is installed on the node. :::","title":"Introduction"},{"location":"elemental-plans/#types-of-plans","text":"The type of plans that Elemental can use are: One time instructions: Only run once Periodic instructions: They run periodically Files: Creates files Probes: http probes :::tip Both one time instructions and periodic instructions can run either a direct command or a docker image. :::","title":"Types of Plans"},{"location":"elemental-plans/#adding-local-plans-on-elemental","text":"You can add local plans to Elemental as part of the MachineRegistration CRD, in the cloud-config section as follows: ```yaml showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: config: cloud-config: users: - name: root passwd: root write_files: - path: /var/lib/elemental/agent/plans/mycustomplan.plan permissions: \"0600\" content: | {\"instructions\": [ { \"name\":\"set hostname\", \"command\":\"hostnamectl\", \"args\": [\"set-hostname\", \"myHostname\"] }, { \"name\":\"stop sshd service\", \"command\":\"systemctl\", \"args\": [\"stop\", \"sshd\"] } ] } elemental: install: reboot: true device: /dev/sda debug: true machineName: my-machine machineInventoryLabels: location: \"europe\" ## Plan examples The following plans are provided as a quick reference and are not guaranteed to work in your environment. To learn more about plans please check [Rancher System Agent](https://github.com/rancher/system-agent). <Tabs> <TabItem value=\"example1\" label=\"Example 1: one time instructions\" default> ```json showLineNumbers {\"instructions\": [ { \"name\":\"set hostname\", \"command\":\"hostnamectl\", \"args\": [\"set-hostname\", \"myHostname\"] }, { \"name\":\"stop sshd service\", \"command\":\"systemctl\", \"args\": [\"stop\", \"sshd\"] } ] } ```json showLineNumbers {\"periodicInstructions\": [ { \"name\":\"set hostname\", \"image\":\"ghcr.io/rancher-sandbox/elemental-example-plan:main\" \"command\": \"run.sh\" } ] } </TabItem> <TabItem value=\"example3\" label=\"Example 3: files\"> ```json showLineNumbers {\"files\": [ { \"content\":\"Welcome to the system\", \"path\":\"/etc/motd\", \"permissions\": \"0644\" } ] } json showLineNumbers {\"probes\": \"probe1\": { \"name\": \"Service Up\", \"httpGet\": { \"url\": \"http://10.0.0.1/healthz\", \"insecure\": \"false\", \"clientCert\": \"....\", \"clientKey\": \"....\", \"caCert\": \".....\" } } }","title":"Adding local plans on Elemental"},{"location":"elementaloperatorchart-reference/","text":"Elemental Operator Helm Chart \u00b6 The is responsible for managing the Elemental versions and maintaining a machine inventory to assist with edge or bare metal installations. The associated chart bootstraps an elemental-operator deployment on the Rancher Manager v2.6 cluster using the Helm package manager. Prerequisites \u00b6 Rancher Manager version v2.6 Helm client version v3.8.0+ Get Helm chart info \u00b6 ```console showLineNumbers helm pull oci://registry.opensuse.org/isv/rancher/elemental/stable/charts/elemental/elemental-operator helm show all oci://registry.opensuse.org/isv/rancher/elemental/stable/charts/elemental/elemental-operator ## Install Chart ```console showLineNumbers helm install --create-namespace -n cattle-elemental-system elemental-operator \\ oci://registry.opensuse.org/isv/rancher/elemental/stable/charts/elemental/elemental-operator The command deploys elemental-operator on the Kubernetes cluster in the default configuration. See configuration below. See helm install for command documentation. Uninstall Chart \u00b6 ```console showLineNumbers helm uninstall -n cattle-elemental-system elemental-operator This removes all the Kubernetes components associated with the chart and deletes the release. _See [helm uninstall](https://helm.sh/docs/helm/helm_uninstall/) for command documentation._ ## Upgrading Chart ```console showLineNumbers helm upgrade -n cattle-elemental-system \\ --install elemental-operator \\ oci://registry.opensuse.org/isv/rancher/elemental/stable/charts/elemental/elemental-operator See helm upgrade for command documentation. Configuration \u00b6 See Customizing the Chart Before Installing . To see all configurable options with detailed comments, visit the chart's values , or run these configuration commands: console showLineNumbers helm show values oci://registry.opensuse.org/isv/rancher/elemental/charts/elemental/elemental-operator Values \u00b6 Key Type Default Description image.empty string rancher/pause:3.1 image.repository string quay.io/costoolkit/elemental-operator Source image for elemental-operator with repository name image.tag tag \"\" image.imagePullPolicy string IfNotPresent noProxy string `127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,.svc,.cluster.local\" Comma separated list of domains or ip addresses that will not use the proxy global.cattle.systemDefaultRegistry string \"\" Default container registry name sync_interval string \"60m\" Default sync interval for upgrade channel sync_namespaces list [] Namespace the operator will watch for, leave empty for all debug bool false Enable debug output for operator nodeSelector.kubernetes.io/os string linux tolerations object {} tolerations.key string cattle.io/os tolerations.operator string \"Equal\" tolerations.value string \"linux\" tolerations.effect string NoSchedule","title":""},{"location":"elementaloperatorchart-reference/#elemental-operator-helm-chart","text":"The is responsible for managing the Elemental versions and maintaining a machine inventory to assist with edge or bare metal installations. The associated chart bootstraps an elemental-operator deployment on the Rancher Manager v2.6 cluster using the Helm package manager.","title":"Elemental Operator Helm Chart"},{"location":"elementaloperatorchart-reference/#prerequisites","text":"Rancher Manager version v2.6 Helm client version v3.8.0+","title":"Prerequisites"},{"location":"elementaloperatorchart-reference/#get-helm-chart-info","text":"```console showLineNumbers helm pull oci://registry.opensuse.org/isv/rancher/elemental/stable/charts/elemental/elemental-operator helm show all oci://registry.opensuse.org/isv/rancher/elemental/stable/charts/elemental/elemental-operator ## Install Chart ```console showLineNumbers helm install --create-namespace -n cattle-elemental-system elemental-operator \\ oci://registry.opensuse.org/isv/rancher/elemental/stable/charts/elemental/elemental-operator The command deploys elemental-operator on the Kubernetes cluster in the default configuration. See configuration below. See helm install for command documentation.","title":"Get Helm chart info"},{"location":"elementaloperatorchart-reference/#uninstall-chart","text":"```console showLineNumbers helm uninstall -n cattle-elemental-system elemental-operator This removes all the Kubernetes components associated with the chart and deletes the release. _See [helm uninstall](https://helm.sh/docs/helm/helm_uninstall/) for command documentation._ ## Upgrading Chart ```console showLineNumbers helm upgrade -n cattle-elemental-system \\ --install elemental-operator \\ oci://registry.opensuse.org/isv/rancher/elemental/stable/charts/elemental/elemental-operator See helm upgrade for command documentation.","title":"Uninstall Chart"},{"location":"elementaloperatorchart-reference/#configuration","text":"See Customizing the Chart Before Installing . To see all configurable options with detailed comments, visit the chart's values , or run these configuration commands: console showLineNumbers helm show values oci://registry.opensuse.org/isv/rancher/elemental/charts/elemental/elemental-operator","title":"Configuration"},{"location":"elementaloperatorchart-reference/#values","text":"Key Type Default Description image.empty string rancher/pause:3.1 image.repository string quay.io/costoolkit/elemental-operator Source image for elemental-operator with repository name image.tag tag \"\" image.imagePullPolicy string IfNotPresent noProxy string `127.0.0.0/8,10.0.0.0/8,172.16.0.0/12,192.168.0.0/16,.svc,.cluster.local\" Comma separated list of domains or ip addresses that will not use the proxy global.cattle.systemDefaultRegistry string \"\" Default container registry name sync_interval string \"60m\" Default sync interval for upgrade channel sync_namespaces list [] Namespace the operator will watch for, leave empty for all debug bool false Enable debug output for operator nodeSelector.kubernetes.io/os string linux tolerations object {} tolerations.key string cattle.io/os tolerations.operator string \"Equal\" tolerations.value string \"linux\" tolerations.effect string NoSchedule","title":"Values"},{"location":"installation/","text":"Installation \u00b6 Overview \u00b6 Elemental stack provides OS management using OCI containers and Kubernetes. The Elemental stack installation encompasses the installation of the into the management cluster and the creation and use of Elemental Teal installation media to provide the OS into the Cluster Nodes. See Architecture section to read about the interaction of the components. The installation configuration is mostly applied and set as part of the registration process. The registration process is done by the elemental-register (the client part) who is the responsible to register nodes in a Rancher management cluster and fetch the installation configuration. Please refer to the Quick Start guide for simple step by step deployment instructions. Elemental Operator Installation \u00b6 The is responsible for managing the Elemental versions and maintaining a machine inventory to assist with edge or bare metal installations. requires a cluster including the Rancher Manager and it can be installed with a helm chart. See helm chart reference for install, uninstall, upgrade and configuration details. Prepare Kubernetes Resources \u00b6 Once the is up and running within the management cluster a couple of kubernetes resources are required in order to prepare an Elemental based cluster deployment. MachineInventorySelectorTemplate : This resource identifies the criteria to match registered boxes (listed as part of the MachineInventory) against available Rancher 2.6 Clusters. As soon as there is a match the selected kubernetes cluster takes ownership of the registered box. MachineRegistration : This resource defines OS deployment details for any machine attempting to register. The machine registration is the entrance for Elemental nodes as it handles the authentication (based on TPM), the Elemental Teal deployment and the node inclusion into to the MachineInventory so it can be added to a cluster when there is a match based on a MachineInventorySelectorTemplate. The MachineRegistration object includes the machine registration URL that nodes use to register against it. A Rancher Cluster resource is also required to deploy Elemental, it can be manually created as exemplified in the Quick Start guide or created from the Rancher 2.6 UI. Prepare Installation Media \u00b6 The installation media is the media that will be used to kick start an Elemental Teal deployment. Currently the supported media is a live ISO. The live ISO must include the registration configuration yaml hence it must crafted once the MachineRegistration is created. The installation media is created by using the elemental-iso-add-registration helper script (see quick start guide) or by using the elemental build-iso command line utility included as part of the . Within MachineRegistration only a subset of OS installation parameters can be configured, all available parameters are listed at MachineRegistration reference page. In order to configure the installation beyond the common options provided within the elemental.install section a config.yaml configuration file can be included into the ISO (see Custom Images ). Note any configuration applied as part of elemental.install section of the MachineRegistration will be applied on top of the settings included in any custom config.yaml file. Most likely the cloud-init configuration is enough to configure and set the deployed node at boot, however if for some reason firstboot actions or scripts are required it is possible to also include Rancher System Agent plans into the installation media. Refer to the Elemental Plans section for details and some example plans. The plans could be included into the squashed rootfs at /var/lib/elemental/agent/plans folder and they would be seen by the system agent at firstboot. Start Installation Process \u00b6 The installation starts by booting the installation media on a node. Once the installation media has booted it will attempt to contact the management cluster and register to it by calling elemental-register command. As the registration yaml configuration is already included into the ISO elemental-register knows the registration URL and any other required data for the registration. On a succeeded registration the installation media will start the Elemental Teal installation into the host based on the configuration already included in the media and the MachineRegistration parameters. As soon as the installation is done the node is ready to reboot. The deployed Elemental Teal includes a system agent plan to kick start a regular rancher provisioning process to install the selected kubernetes version, once booted, after some minutes the node installation is finalized and the node is included into the cluster and visible through the Rancher UI. Deployed Elemental Teal Partition Table \u00b6 Once Elemental Teal is installed the OS partition table, according to default values, will look like Label Default Size Contains COS_BOOT 64 MiB UEFI Boot partition COS_STATE 15 GiB A/B bootable file system images constructed from OCI images COS_OEM 64 MiB OEM cloud-config files and other data COS_RECOVERY 8 GiB Recovery file system image if COS_STATE is destroyed COS_PERSISTENT Remaining space All contents of the persistent folders Note this is the basic structure of any OS built by the Elemental Teal Immutable Root \u00b6 One of the characteristics of Elemental OSes is the setup of an immutable root filesystem where some ephemeral or persistent locations are applied on top of it. Elemental Teal default folders structure is listed in the matrix below. Path Read-Only Ephemeral Persistent / x /etc x /etc/cni x /etc/iscsi x /etc/rancher x /etc/ssh x /etc/systemd x /srv x /home x /opt x /root x /var x /usr/libexec x /var/lib/cni x /var/lib/kubelet x /var/lib/longhorn x /var/lib/rancher x /var/lib/elemetal x /var/lib/wicked x /var/lib/calico x /var/log x","title":""},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#overview","text":"Elemental stack provides OS management using OCI containers and Kubernetes. The Elemental stack installation encompasses the installation of the into the management cluster and the creation and use of Elemental Teal installation media to provide the OS into the Cluster Nodes. See Architecture section to read about the interaction of the components. The installation configuration is mostly applied and set as part of the registration process. The registration process is done by the elemental-register (the client part) who is the responsible to register nodes in a Rancher management cluster and fetch the installation configuration. Please refer to the Quick Start guide for simple step by step deployment instructions.","title":"Overview"},{"location":"installation/#elemental-operator-installation","text":"The is responsible for managing the Elemental versions and maintaining a machine inventory to assist with edge or bare metal installations. requires a cluster including the Rancher Manager and it can be installed with a helm chart. See helm chart reference for install, uninstall, upgrade and configuration details.","title":"Elemental Operator Installation"},{"location":"installation/#prepare-kubernetes-resources","text":"Once the is up and running within the management cluster a couple of kubernetes resources are required in order to prepare an Elemental based cluster deployment. MachineInventorySelectorTemplate : This resource identifies the criteria to match registered boxes (listed as part of the MachineInventory) against available Rancher 2.6 Clusters. As soon as there is a match the selected kubernetes cluster takes ownership of the registered box. MachineRegistration : This resource defines OS deployment details for any machine attempting to register. The machine registration is the entrance for Elemental nodes as it handles the authentication (based on TPM), the Elemental Teal deployment and the node inclusion into to the MachineInventory so it can be added to a cluster when there is a match based on a MachineInventorySelectorTemplate. The MachineRegistration object includes the machine registration URL that nodes use to register against it. A Rancher Cluster resource is also required to deploy Elemental, it can be manually created as exemplified in the Quick Start guide or created from the Rancher 2.6 UI.","title":"Prepare Kubernetes Resources"},{"location":"installation/#prepare-installation-media","text":"The installation media is the media that will be used to kick start an Elemental Teal deployment. Currently the supported media is a live ISO. The live ISO must include the registration configuration yaml hence it must crafted once the MachineRegistration is created. The installation media is created by using the elemental-iso-add-registration helper script (see quick start guide) or by using the elemental build-iso command line utility included as part of the . Within MachineRegistration only a subset of OS installation parameters can be configured, all available parameters are listed at MachineRegistration reference page. In order to configure the installation beyond the common options provided within the elemental.install section a config.yaml configuration file can be included into the ISO (see Custom Images ). Note any configuration applied as part of elemental.install section of the MachineRegistration will be applied on top of the settings included in any custom config.yaml file. Most likely the cloud-init configuration is enough to configure and set the deployed node at boot, however if for some reason firstboot actions or scripts are required it is possible to also include Rancher System Agent plans into the installation media. Refer to the Elemental Plans section for details and some example plans. The plans could be included into the squashed rootfs at /var/lib/elemental/agent/plans folder and they would be seen by the system agent at firstboot.","title":"Prepare Installation Media"},{"location":"installation/#start-installation-process","text":"The installation starts by booting the installation media on a node. Once the installation media has booted it will attempt to contact the management cluster and register to it by calling elemental-register command. As the registration yaml configuration is already included into the ISO elemental-register knows the registration URL and any other required data for the registration. On a succeeded registration the installation media will start the Elemental Teal installation into the host based on the configuration already included in the media and the MachineRegistration parameters. As soon as the installation is done the node is ready to reboot. The deployed Elemental Teal includes a system agent plan to kick start a regular rancher provisioning process to install the selected kubernetes version, once booted, after some minutes the node installation is finalized and the node is included into the cluster and visible through the Rancher UI.","title":"Start Installation Process"},{"location":"installation/#deployed-elemental-teal-partition-table","text":"Once Elemental Teal is installed the OS partition table, according to default values, will look like Label Default Size Contains COS_BOOT 64 MiB UEFI Boot partition COS_STATE 15 GiB A/B bootable file system images constructed from OCI images COS_OEM 64 MiB OEM cloud-config files and other data COS_RECOVERY 8 GiB Recovery file system image if COS_STATE is destroyed COS_PERSISTENT Remaining space All contents of the persistent folders Note this is the basic structure of any OS built by the","title":"Deployed Elemental Teal Partition Table"},{"location":"installation/#elemental-teal-immutable-root","text":"One of the characteristics of Elemental OSes is the setup of an immutable root filesystem where some ephemeral or persistent locations are applied on top of it. Elemental Teal default folders structure is listed in the matrix below. Path Read-Only Ephemeral Persistent / x /etc x /etc/cni x /etc/iscsi x /etc/rancher x /etc/ssh x /etc/systemd x /srv x /home x /opt x /root x /var x /usr/libexec x /var/lib/cni x /var/lib/kubelet x /var/lib/longhorn x /var/lib/rancher x /var/lib/elemetal x /var/lib/wicked x /var/lib/calico x /var/log x","title":"Elemental Teal Immutable Root"},{"location":"inventory-management/","text":"Inventory Management \u00b6 The Elemental operator can hold an inventory of machines and the mapping of the machine to it's configuration and assigned cluster. MachineInventory \u00b6 Reference \u00b6 apiVersion : elemental.cattle.io/v1beta1 kind : MachineInventory metadata : name : machine-a # The namespace must match the namespace of the cluster # assigned to the clusters.provisioning.cattle.io resource namespace : fleet-default spec : # The cluster that this machine is assigned to clusterName : some-cluster # The hash of the TPM EK public key. This is used if you are # using TPM2 to identifiy nodes. You can obtain the TPM by # running `rancherd get-tpm-hash` on the node. Or nodes can # report their TPM hash by using the MachineRegister tpm : d68795c6192af9922692f050b... # Generic SMBIOS fields that are typically populated with # the MachineRegister approach smbios : {} # A reference to a secret that contains a shared secret value to # identify a node. The secret must be of type \"elemental.cattle.io/token\" # and have on field \"token\" which is the value of the shared secret machineTokenSecretName : some-secret-name # Arbitrary cloud config that will be added to the machines cloud config # during the rancherd bootstrap phase. The one important field that should # be set is the role. config : role : server MachineRegistration \u00b6 Reference \u00b6 apiVersion : elemental.cattle.io/v1beta1 kind : MachineRegistration metadata : name : machine-registration # The namespace must match the namespace of the cluster # assigned to the clusters.provisioning.cattle.io resource namespace : fleet-default spec : # Labels to be added to the created MachineInventory object machineInventoryLabels : {} # Annotations to be added to the created MachineInventory object machineInventoryAnnotations : {} # The cloud config that will be used to provision the node cloudConfig : {}","title":""},{"location":"inventory-management/#inventory-management","text":"The Elemental operator can hold an inventory of machines and the mapping of the machine to it's configuration and assigned cluster.","title":"Inventory Management"},{"location":"inventory-management/#machineinventory","text":"","title":"MachineInventory"},{"location":"inventory-management/#reference","text":"apiVersion : elemental.cattle.io/v1beta1 kind : MachineInventory metadata : name : machine-a # The namespace must match the namespace of the cluster # assigned to the clusters.provisioning.cattle.io resource namespace : fleet-default spec : # The cluster that this machine is assigned to clusterName : some-cluster # The hash of the TPM EK public key. This is used if you are # using TPM2 to identifiy nodes. You can obtain the TPM by # running `rancherd get-tpm-hash` on the node. Or nodes can # report their TPM hash by using the MachineRegister tpm : d68795c6192af9922692f050b... # Generic SMBIOS fields that are typically populated with # the MachineRegister approach smbios : {} # A reference to a secret that contains a shared secret value to # identify a node. The secret must be of type \"elemental.cattle.io/token\" # and have on field \"token\" which is the value of the shared secret machineTokenSecretName : some-secret-name # Arbitrary cloud config that will be added to the machines cloud config # during the rancherd bootstrap phase. The one important field that should # be set is the role. config : role : server","title":"Reference"},{"location":"inventory-management/#machineregistration","text":"","title":"MachineRegistration"},{"location":"inventory-management/#reference_1","text":"apiVersion : elemental.cattle.io/v1beta1 kind : MachineRegistration metadata : name : machine-registration # The namespace must match the namespace of the cluster # assigned to the clusters.provisioning.cattle.io resource namespace : fleet-default spec : # Labels to be added to the created MachineInventory object machineInventoryLabels : {} # Annotations to be added to the created MachineInventory object machineInventoryAnnotations : {} # The cloud config that will be used to provision the node cloudConfig : {}","title":"Reference"},{"location":"kubernetesversions/","text":"Valid Versions \u00b6 The list of valid versions for the kubernetesVersion field can be determined from the Rancher metadata using the following commands. k3s: ```bash showLineNumbers curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.k3s.releases[].version' __rke2:__ ```bash showLineNumbers curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.rke2.releases[].version'","title":""},{"location":"kubernetesversions/#valid-versions","text":"The list of valid versions for the kubernetesVersion field can be determined from the Rancher metadata using the following commands. k3s: ```bash showLineNumbers curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.k3s.releases[].version' __rke2:__ ```bash showLineNumbers curl -sL https://raw.githubusercontent.com/rancher/kontainer-driver-metadata/release-v2.6/data/data.json | jq -r '.rke2.releases[].version'","title":"Valid Versions"},{"location":"machineinventoryselectortemplate-reference/","text":"MachineInventorySelectorTemplate reference \u00b6 The MachineInventorySelectorTemplate is the resource responsible of defining the matching criteria to pair an inventoried machine with a Cluster resource. The relevant key is the selector which includes label selector expressions. yaml title=\"MachineInventorySelectorTemplate\" showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineInventorySelectorTemplate metadata: name: my-machine-selector namespace: fleet-default spec: template: spec: selector: ... template.spec.selector can include matchLabels and or matchExpressions keys. template.spec.selector.matchLabels \u00b6 It is a map of {key,value} pairs (map[string]string). When multiple labels are provided all labels must match. Example ```yaml showLineNumbers ... spec: template: spec: selector: matchlabels: location: europe manufacturer: somevalue ``` A Cluster defined with the above selector will only attempt to provision nodes inventoried including these two labels. template.spec.selector.matchExpressions \u00b6 It is a list of label selectors, each label selectors can be defined as: Key Type Description key string This is the label key the selector applies on operator string Represents the relationship of the key to a set of values. Valid operators are 'In', 'NotIn', 'Exists' and 'DoesNotExist' values []string Values is an array of string values. If the operator is 'In' or 'NotIn', the values array must be non-empty. If the operator is 'Exists' or 'DoesNotExist', the values array must be empty Example ```yaml showLineNumbers ... spec: template: spec: selector: matchExpressions: - key: location operator: In values: [ 'europe' ] - key: manufacturer operator: Exists ``` A Cluster defined with the above selector will only attempt to provision nodes inventoried with the location=europe label and including a manufacturer label defined with any value.","title":""},{"location":"machineinventoryselectortemplate-reference/#machineinventoryselectortemplate-reference","text":"The MachineInventorySelectorTemplate is the resource responsible of defining the matching criteria to pair an inventoried machine with a Cluster resource. The relevant key is the selector which includes label selector expressions. yaml title=\"MachineInventorySelectorTemplate\" showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineInventorySelectorTemplate metadata: name: my-machine-selector namespace: fleet-default spec: template: spec: selector: ... template.spec.selector can include matchLabels and or matchExpressions keys.","title":"MachineInventorySelectorTemplate reference"},{"location":"machineinventoryselectortemplate-reference/#templatespecselectormatchlabels","text":"It is a map of {key,value} pairs (map[string]string). When multiple labels are provided all labels must match. Example ```yaml showLineNumbers ... spec: template: spec: selector: matchlabels: location: europe manufacturer: somevalue ``` A Cluster defined with the above selector will only attempt to provision nodes inventoried including these two labels.","title":"template.spec.selector.matchLabels"},{"location":"machineinventoryselectortemplate-reference/#templatespecselectormatchexpressions","text":"It is a list of label selectors, each label selectors can be defined as: Key Type Description key string This is the label key the selector applies on operator string Represents the relationship of the key to a set of values. Valid operators are 'In', 'NotIn', 'Exists' and 'DoesNotExist' values []string Values is an array of string values. If the operator is 'In' or 'NotIn', the values array must be non-empty. If the operator is 'Exists' or 'DoesNotExist', the values array must be empty Example ```yaml showLineNumbers ... spec: template: spec: selector: matchExpressions: - key: location operator: In values: [ 'europe' ] - key: manufacturer operator: Exists ``` A Cluster defined with the above selector will only attempt to provision nodes inventoried with the location=europe label and including a manufacturer label defined with any value.","title":"template.spec.selector.matchExpressions"},{"location":"machineregistration-reference/","text":"MachineRegistration reference \u00b6 The MachineRegistration resource is the responsible of defining a machine registration end point. Once created it generates a registration URL used by nodes to register so they are inventoried. There are several keys that can be configured under a MachineRegistration resource spec. There are several keys that can be configured under a MachineRegistration resource spec. yaml title=\"MachineRegistration\" showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: machineName: name machineInventoryLabels: label: value machineInventoryAnnotations: annotation: value config: cloud-config: ... elemental: registration: ... install: ... config.cloud-config \u00b6 Contains the cloud-configuration to be injected in the node. See the Cloud Config Reference for full information. config.elemental.registration \u00b6 Contains the configuration used for the connection and the initial registration to the . Supports the following values: Key Type Default value Description url string empty URL to connect to the ca-cert string empty CA to validate the certificate provided by the server at 'url' (required if the certificate is not signed by a public CA) emulate-tpm bool false this will use software emulation of the TPM (required for hosts without TPM hardware) emulated-tpm-seed int64 1 fixed seed to use with 'emulate-tpm': use for debug purposes only no-smbios bool false whether SMBIOS data should be sent to the (see the SMBIOS reference for more information) config.elemental.install \u00b6 Contains the installation configuration that would be applied via operator-register when booted from an ISO and passed to elemental-cli install Supports the following values: Key Type Default value Description firmware string efi Firmware to install ('efi' or 'bios') device string empty Device to install the system to no-format bool false Don\u2019t format disks. It is implied that COS_STATE, COS_RECOVERY, COS_PERSISTENT, COS_OEM partitions are already existing on the target disk config-urls list empty Cloud-init config files locations iso string empty Performs an installation from the ISO url instead of the running ISO system-uri string empty Sets the system image source and its type (e.g. 'docker:registry.org/image:tag') instead of using the running ISO debug bool false Enable debug output tty string empty Add named tty to grub poweroff bool false Shutdown the system after install reboot bool false Reboot the system after install eject-cd bool false Try to eject the cd on reboot :::warning warning In case of using both iso and system-uri the iso value takes precedence ::: The only required value for a successful installation is the device key as we need a target disk to install to. The rest of the parameters are all optional. Example ```yaml showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: config: elemental: install: device: /dev/sda debug: true reboot: true eject-cd: true system-uri: registry.opensuse.org/isv/rancher/elemental/teal52/15.3/rancher/elemental-node-image/5.2:latest ``` machineName \u00b6 This refers to the name that will be set to the node and the kubernetes resources that require a hostname (rke2 deployed pods for example, they use the node hostname as part of the pod names) String type. :::info When elemental:registration:no-smbios is set to false (default), machineName is interpolated with SMBIOS data which allows you to store hardware information. See our SMBIOS docs for more information. If no machineName is specified, a default one in the form m-$UUID will be set. The UUID will be retrieved from the SMBIOS data if available, otherwise a random UUID will be generated. ::: Example ```yaml showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: machineName: hostname-test-4 ``` machineInventoryLabels \u00b6 Labels that will be set to the MachineInventory that is created from this MachineRegistration Key: value type. These labels will be used to establish a selection criteria in MachineInventorySelectorTemplate . :::info When elemental:registration:no-smbios is set to false (default), Labels are interpolated with SMBIOS data. This allows to store hardware information in custom labels. See our SMBIOS docs for more information. ::: Example ```yaml showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: machineInventoryLabels: my.prefix.io/location: europe my.prefix.io/cpus: 32 my.prefix.io/manufacturer: \"${System Information/Manufacturer}\" my.prefix.io/productName: \"${System Information/Product Name}\" my.prefix.io/serialNumber: \"${System Information/Serial Number}\" my.prefix.io/machineUUID: \"${System Information/UUID}\" ``` machineInventoryAnnotations \u00b6 Annotations that will be set to the MachineInventory that is created from this MachineRegistration Key: value type Example ```yaml showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: machineInventoryAnnotations: owner: bob version: 1.0.0 ```","title":""},{"location":"machineregistration-reference/#machineregistration-reference","text":"The MachineRegistration resource is the responsible of defining a machine registration end point. Once created it generates a registration URL used by nodes to register so they are inventoried. There are several keys that can be configured under a MachineRegistration resource spec. There are several keys that can be configured under a MachineRegistration resource spec. yaml title=\"MachineRegistration\" showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: machineName: name machineInventoryLabels: label: value machineInventoryAnnotations: annotation: value config: cloud-config: ... elemental: registration: ... install: ...","title":"MachineRegistration reference"},{"location":"machineregistration-reference/#configcloud-config","text":"Contains the cloud-configuration to be injected in the node. See the Cloud Config Reference for full information.","title":"config.cloud-config"},{"location":"machineregistration-reference/#configelementalregistration","text":"Contains the configuration used for the connection and the initial registration to the . Supports the following values: Key Type Default value Description url string empty URL to connect to the ca-cert string empty CA to validate the certificate provided by the server at 'url' (required if the certificate is not signed by a public CA) emulate-tpm bool false this will use software emulation of the TPM (required for hosts without TPM hardware) emulated-tpm-seed int64 1 fixed seed to use with 'emulate-tpm': use for debug purposes only no-smbios bool false whether SMBIOS data should be sent to the (see the SMBIOS reference for more information)","title":"config.elemental.registration"},{"location":"machineregistration-reference/#configelementalinstall","text":"Contains the installation configuration that would be applied via operator-register when booted from an ISO and passed to elemental-cli install Supports the following values: Key Type Default value Description firmware string efi Firmware to install ('efi' or 'bios') device string empty Device to install the system to no-format bool false Don\u2019t format disks. It is implied that COS_STATE, COS_RECOVERY, COS_PERSISTENT, COS_OEM partitions are already existing on the target disk config-urls list empty Cloud-init config files locations iso string empty Performs an installation from the ISO url instead of the running ISO system-uri string empty Sets the system image source and its type (e.g. 'docker:registry.org/image:tag') instead of using the running ISO debug bool false Enable debug output tty string empty Add named tty to grub poweroff bool false Shutdown the system after install reboot bool false Reboot the system after install eject-cd bool false Try to eject the cd on reboot :::warning warning In case of using both iso and system-uri the iso value takes precedence ::: The only required value for a successful installation is the device key as we need a target disk to install to. The rest of the parameters are all optional. Example ```yaml showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: config: elemental: install: device: /dev/sda debug: true reboot: true eject-cd: true system-uri: registry.opensuse.org/isv/rancher/elemental/teal52/15.3/rancher/elemental-node-image/5.2:latest ```","title":"config.elemental.install"},{"location":"machineregistration-reference/#machinename","text":"This refers to the name that will be set to the node and the kubernetes resources that require a hostname (rke2 deployed pods for example, they use the node hostname as part of the pod names) String type. :::info When elemental:registration:no-smbios is set to false (default), machineName is interpolated with SMBIOS data which allows you to store hardware information. See our SMBIOS docs for more information. If no machineName is specified, a default one in the form m-$UUID will be set. The UUID will be retrieved from the SMBIOS data if available, otherwise a random UUID will be generated. ::: Example ```yaml showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: machineName: hostname-test-4 ```","title":"machineName"},{"location":"machineregistration-reference/#machineinventorylabels","text":"Labels that will be set to the MachineInventory that is created from this MachineRegistration Key: value type. These labels will be used to establish a selection criteria in MachineInventorySelectorTemplate . :::info When elemental:registration:no-smbios is set to false (default), Labels are interpolated with SMBIOS data. This allows to store hardware information in custom labels. See our SMBIOS docs for more information. ::: Example ```yaml showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: machineInventoryLabels: my.prefix.io/location: europe my.prefix.io/cpus: 32 my.prefix.io/manufacturer: \"${System Information/Manufacturer}\" my.prefix.io/productName: \"${System Information/Product Name}\" my.prefix.io/serialNumber: \"${System Information/Serial Number}\" my.prefix.io/machineUUID: \"${System Information/UUID}\" ```","title":"machineInventoryLabels"},{"location":"machineregistration-reference/#machineinventoryannotations","text":"Annotations that will be set to the MachineInventory that is created from this MachineRegistration Key: value type Example ```yaml showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: machineInventoryAnnotations: owner: bob version: 1.0.0 ```","title":"machineInventoryAnnotations"},{"location":"quickstart/","text":"import Cluster from \"!!raw-loader!../examples/quickstart/cluster.yaml\" import Registration from \"!!raw-loader!../examples/quickstart/registration.yaml\" import Selector from \"!!raw-loader!../examples/quickstart/selector.yaml\" Quickstart \u00b6 Follow this guide to have an auto-deployed cluster via rke2/k3s and managed by Rancher with the only help of an Elemental Teal iso Introduction \u00b6 What is Elemental Teal ? \u00b6 Elemental Teal is the combination of \"SLE Micro for Rancher\" with the Rancher Elemental stack SLE Micro for Rancher is a containerized and \"stripped to the bones\" operating system layer. It only requires grub2, dracut, a kernel, and systemd. Its sole purpose is to run Kubernetes (k3s or RKE2), with everything controlled through Rancher Manager. Elemental Teal is built in the openSUSE Build Service and available through the openSUSE Registry What is the Rancher Elemental Stack ? \u00b6 The Elemental Stack consists of some packages on top of SLE Micro for Rancher elemental-toolkit - includes a set of OS utilities to enable OS management via containers. Includes dracut modules, bootloader configuration, cloud-init style configuration services, etc. elemental-operator - this connects to Rancher Manager and handles machineRegistration and machineInventory CRDs elemental-register - this registers machines via machineRegistrations and installs them via elemental-cli elemental-cli - this installs any elemental-toolkit based derivative. Basically an installer based on our A/B install and upgrade system rancher-system-agent - runs on the installed system and gets instructions (\"Plans\") from Rancher Manager what to install and run on the system Prerequisites \u00b6 A Rancher server (2.6.9) configured (server-url set) To configure the Rancher server-url please check the Rancher docs A machine (bare metal or virtualized) with TPM 2.0 Hint 1: Libvirt allows setting virtual TPMs for virtual machines example here Hint 2: You can enable TPM emulation on bare metal machines missing the TPM 2.0 module example here Helm Package Manager (https://helm.sh/) Docker (for iso manipulation) Preparing the cluster \u00b6 elemental-operator is the management endpoint, running the management cluster and taking care of creating inventories, registrations for machines and much more. We will use the Helm package manager to install the elemental-operator chart into our cluster ```shell showLineNumbers helm upgrade --create-namespace -n cattle-elemental-system --install elemental-operator oci://registry.opensuse.org/isv/rancher/elemental/stable/charts/elemental/elemental-operator There is a few options that can be set in the chart install but that is out of scope for this document. You can see all the values on the chart [values.yaml](https://github.com/rancher/elemental-operator/blob/main/chart/values.yaml) Now after a few seconds you should see the operator pod appear on the `cattle-elemental-system` namespace. ```shell showLineNumbers kubectl get pods -n cattle-elemental-system NAME READY STATUS RESTARTS AGE elemental-operator-64f88fc695-b8qhn 1/1 Running 0 16s Prepare you kubernetes resources \u00b6 Node deployment starts with a MachineRegistration , identifying a set of machines sharing the same configuration (disk drives, network, etc.) Then it continues with having a Cluster resource that uses a MachineInventorySelectorTemplate to know which machines are for that cluster. This selector is a simple matcher based on labels set in the MachineInventory , so if your selector is matching the cluster-id key with a value myId and your MachineInventory has that same key with that value, it will match and be bootstrapped as part of the cluster. You will need to create the following files. {Selector} As you can see this is a very simple selector that checks the key node-location for the value europe {Cluster} As you can see we are setting that our machineConfigRef is of Kind MachineInventorySelectorTemplate with the name my-machine-selector , which matches the selector we created. {Registration} This creates a MachineRegistration which will provide a unique URL which we will use with elemental-register to register the node during installation, so the operator can create a MachineInventory which will be using to bootstrap the node. See that we set the label that match our selector here already, although it can always be added later to the MachineInventory . :::warning warning Make sure to modify the registration.yaml above to set the proper install device to point to a valid device based on your node configuration(i.e. /dev/sda, /dev/vda, /dev/nvme0, etc...) ::: Now that we have all the configuration to create the proper resources in Kubernetes just apply them ```shell showLineNumbers kubectl apply -f selector.yaml kubectl apply -f cluster.yaml kubectl apply -f registration.yaml </TabItem> <TabItem value=\"repofiles\" label=\"Using quickstart files from Elemental repo directly\"> You can directly apply the quickstart example resource files from the [Elemental repository](https://github.com/rancher/elemental) :::warning warning This assumes that your Node will have a `/dev/sda` disk available as that is the default device selected in those files. If your node doesnt have that device you will have to manually create the registration.yaml file or download the one from the repo and modify before applying ::: ```bash showLineNumbers kubectl apply -f https://raw.githubusercontent.com/rancher/elemental/main/examples/quickstart/selector.yaml kubectl apply -f https://raw.githubusercontent.com/rancher/elemental/main/examples/quickstart/cluster.yaml kubectl apply -f https://raw.githubusercontent.com/rancher/elemental/main/examples/quickstart/registration.yaml Preparing the iso \u00b6 Now this is the last step, we need to prepare an Elemental Teal iso that includes the initial registration config, so it can be auto registered, installed and fully deployed as part of our cluster. The contents of the file are nothing more than the registration url that the node needs to register and the proper server certificate, so it can connect securely. This iso then can be used to provision an infinite number of machines Now, our MachineRegistration provides the needed config in its resource as part of its Status.RegistrationURL , so we can use that url to obtain the proper yaml needed for the iso. ``shell showLineNumbers wget --no-check-certificate kubectl get machineregistration -n fleet-default my-nodes -o jsonpath=\"{.status.registrationURL}\"` -O initial-registration.yaml This will download the proper yaml from the registration URL and store it on the current directory under the `initial-registration.yaml` name. </TabItem> <TabItem value=\"explanation\" label=\"Full explanation\"> First we need to obtain the `RegistrationURL` that was generated for our `MachineRegistration` ```bash showLineNumbers $ kubectl get machineregistration -n fleet-default my-test-registration -o jsonpath=\"{.status.registrationURL}\" https://172.18.0.2.sslip.io/elemental/registration/gsh4n8nj9gvbsjk4x7hxvnr5l6hmhbdbdffrmkwzrss2dtfbnpbmqp As you can see we obtained the proper initial registration needed by elemental-register to register the node properly and continue with the automated installation Then we need to visit that URL as that will provide the URL and CA certificate for unauthenticated requests: ```bash showLineNumbers $ curl --insecure https://172.18.0.2.sslip.io/elemental/registration/gsh4n8nj9gvbsjk4x7hxvnr5l6hmhbdbdffrmkwzrss2dtfbnpbmqp elemental: registration: url: https://172.18.0.2.sslip.io/elemental/registration/gsh4n8nj9gvbsjk4x7hxvnr5l6hmhbdbdffrmkwzrss2dtfbnpbmqp ca-cert: |- -----BEGIN CERTIFICATE----- MIIBqDCCAU2gAwIBAgIBADAKBggqhkjOPQQDAjA7MRwwGgYDVQQKExNkeW5hbWlj bGlzdGVuZXItb3JnMRswGQYDVQQDExJkeW5hbWljbGlzdGVuZXItY2EwHhcNMjIw ODA0MTA1OTE1WhcNMzIwODAxMTA1OTE1WjA7MRwwGgYDVQQKExNkeW5hbWljbGlz dGVuZXItb3JnMRswGQYDVQQDExJkeW5hbWljbGlzdGVuZXItY2EwWTATBgcqhkjO PQIBBggqhkjOPQMBBwNCAASa8PJH7JJGT5QUPMBYnJe0j50G7dTEaDlk4xRpqVk1 y4dloslsI0RTb6B++7nNgnLPOe2KqZfylNmVIAelrSaUo0IwQDAOBgNVHQ8BAf8E BAMCAqQwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUxp8OBfjZlnyV6pzzKqIF wWByvCYwCgYIKoZIzj0EAwIDSQAwRgIhAPI2XUWcnxkkBe98SGPFa1Hlncyu/FCR AbEYIAdUC2z+AiEA+GizukSRiiLV28wdNdKihEELy+qzi5MlVYowUuQYZsA= -----END CERTIFICATE----- As you can see we obtained the proper initial registration needed by `elemental-register` to register the node properly and continue with the automated installation Now we can write down the data returned for that url into a file that we will inject into the iso ```yaml title=\"initial-registration.yaml\" showLineNumbers elemental: registration: url: https://172.18.0.2.sslip.io/elemental/registration/gsh4n8nj9gvbsjk4x7hxvnr5l6hmhbdbdffrmkwzrss2dtfbnpbmqp ca-cert: |- -----BEGIN CERTIFICATE----- MIIBqDCCAU2gAwIBAgIBADAKBggqhkjOPQQDAjA7MRwwGgYDVQQKExNkeW5hbWlj bGlzdGVuZXItb3JnMRswGQYDVQQDExJkeW5hbWljbGlzdGVuZXItY2EwHhcNMjIw ODA0MTA1OTE1WhcNMzIwODAxMTA1OTE1WjA7MRwwGgYDVQQKExNkeW5hbWljbGlz dGVuZXItb3JnMRswGQYDVQQDExJkeW5hbWljbGlzdGVuZXItY2EwWTATBgcqhkjO PQIBBggqhkjOPQMBBwNCAASa8PJH7JJGT5QUPMBYnJe0j50G7dTEaDlk4xRpqVk1 y4dloslsI0RTb6B++7nNgnLPOe2KqZfylNmVIAelrSaUo0IwQDAOBgNVHQ8BAf8E BAMCAqQwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUxp8OBfjZlnyV6pzzKqIF wWByvCYwCgYIKoZIzj0EAwIDSQAwRgIhAPI2XUWcnxkkBe98SGPFa1Hlncyu/FCR AbEYIAdUC2z+AiEA+GizukSRiiLV28wdNdKihEELy+qzi5MlVYowUuQYZsA= -----END CERTIFICATE----- Now we can proceed to create the ISO We provide a ISO build script for ease of use that can get the final ISO and inject the initial-registration.yaml : ```shell showLineNumbers wget -q https://raw.githubusercontent.com/rancher/elemental/main/.github/elemental-iso-add-registration && chmod +x elemental-iso-add-registration Now that we have the script we can proceed to download the ISO and inject our configuration injected: ```shell showLineNumbers ./elemental-iso-add-registration initial-registration.yaml This will generate an ISO on the current directory with the name elemental-teal-<ARCH>.iso :::info The script uses the iso for the arch based on the system is being run from. If you want to cross build for another system, you can set the ARCH environment variable to the desired target system (x86_64, aarch64) and the iso will be build for that architecture. ::: ```shell showLineNumbers wget -q https://raw.githubusercontent.com/rancher/elemental/main/elemental-iso-build && chmod +x elemental-iso-build Now that we have the script we can proceed to build the ISO with our configuration injected: ```shell showLineNumbers ./elemental-iso-build initial-registration.yaml This will generate an ISO on the current directory with the name elemental-<timestamp>.iso You can now boot your nodes with this ISO, and they will: Boot from the ISO Register with the registrationURL given and create a per-machine MachineInventory Install Elemental Teal to the given device Restart Auto-deploy the cluster via k3s After a few minutes your new cluster will be fully provisioned!! How can I choose the kubernetes version and deployer for the cluster? \u00b6 On you cluster.yaml file there is a key in the Spec called kubernetesVersion . That sets the version and deployer that will be used for the cluster, for example for rke v1.23.6 while for rke2 would be v1.23.6+rke2r1 and for k3s v1.23.6+k3s1 To see all compatible versions check the Rancher Support Matrix PDF for rke/rke2/k3s versions and their components. You can also check our Version doc to know how to obtain those versions. Check our Cluster Spec page for more info about the Cluster resource. How can I follow what is going on behind the scenes? \u00b6 You should be able to follow along what the machine is doing via: During ISO boot: ssh into the machine (user/pass: root/ros): running journalctl -f -t elemental will show you the output of the elemental-register and the elemental install Once the system is installed: On the Rancher UI -> Cluster Management you should see your new cluster and be able to see the Provisioning Log in the cluster details ssh into the machine (user/pass: Whatever your configured on the registration.yaml under Spec.config.cloud-config.users ): running journalctl -f -u elemental-system-agent will show the output of the initial elemental config and install of rancher-system-agent running journalctl -f -u rancher-system-agent will show the output of the boostrap of cluster components like k3s running journalctl -f -u k3s will show the logs of the k3s deployment","title":"Quickstart"},{"location":"quickstart/#quickstart","text":"Follow this guide to have an auto-deployed cluster via rke2/k3s and managed by Rancher with the only help of an Elemental Teal iso","title":"Quickstart"},{"location":"quickstart/#introduction","text":"","title":"Introduction"},{"location":"quickstart/#what-is-elemental-teal","text":"Elemental Teal is the combination of \"SLE Micro for Rancher\" with the Rancher Elemental stack SLE Micro for Rancher is a containerized and \"stripped to the bones\" operating system layer. It only requires grub2, dracut, a kernel, and systemd. Its sole purpose is to run Kubernetes (k3s or RKE2), with everything controlled through Rancher Manager. Elemental Teal is built in the openSUSE Build Service and available through the openSUSE Registry","title":"What is Elemental Teal ?"},{"location":"quickstart/#what-is-the-rancher-elemental-stack","text":"The Elemental Stack consists of some packages on top of SLE Micro for Rancher elemental-toolkit - includes a set of OS utilities to enable OS management via containers. Includes dracut modules, bootloader configuration, cloud-init style configuration services, etc. elemental-operator - this connects to Rancher Manager and handles machineRegistration and machineInventory CRDs elemental-register - this registers machines via machineRegistrations and installs them via elemental-cli elemental-cli - this installs any elemental-toolkit based derivative. Basically an installer based on our A/B install and upgrade system rancher-system-agent - runs on the installed system and gets instructions (\"Plans\") from Rancher Manager what to install and run on the system","title":"What is the Rancher Elemental Stack ?"},{"location":"quickstart/#prerequisites","text":"A Rancher server (2.6.9) configured (server-url set) To configure the Rancher server-url please check the Rancher docs A machine (bare metal or virtualized) with TPM 2.0 Hint 1: Libvirt allows setting virtual TPMs for virtual machines example here Hint 2: You can enable TPM emulation on bare metal machines missing the TPM 2.0 module example here Helm Package Manager (https://helm.sh/) Docker (for iso manipulation)","title":"Prerequisites"},{"location":"quickstart/#preparing-the-cluster","text":"elemental-operator is the management endpoint, running the management cluster and taking care of creating inventories, registrations for machines and much more. We will use the Helm package manager to install the elemental-operator chart into our cluster ```shell showLineNumbers helm upgrade --create-namespace -n cattle-elemental-system --install elemental-operator oci://registry.opensuse.org/isv/rancher/elemental/stable/charts/elemental/elemental-operator There is a few options that can be set in the chart install but that is out of scope for this document. You can see all the values on the chart [values.yaml](https://github.com/rancher/elemental-operator/blob/main/chart/values.yaml) Now after a few seconds you should see the operator pod appear on the `cattle-elemental-system` namespace. ```shell showLineNumbers kubectl get pods -n cattle-elemental-system NAME READY STATUS RESTARTS AGE elemental-operator-64f88fc695-b8qhn 1/1 Running 0 16s","title":"Preparing the cluster"},{"location":"quickstart/#prepare-you-kubernetes-resources","text":"Node deployment starts with a MachineRegistration , identifying a set of machines sharing the same configuration (disk drives, network, etc.) Then it continues with having a Cluster resource that uses a MachineInventorySelectorTemplate to know which machines are for that cluster. This selector is a simple matcher based on labels set in the MachineInventory , so if your selector is matching the cluster-id key with a value myId and your MachineInventory has that same key with that value, it will match and be bootstrapped as part of the cluster. You will need to create the following files. {Selector} As you can see this is a very simple selector that checks the key node-location for the value europe {Cluster} As you can see we are setting that our machineConfigRef is of Kind MachineInventorySelectorTemplate with the name my-machine-selector , which matches the selector we created. {Registration} This creates a MachineRegistration which will provide a unique URL which we will use with elemental-register to register the node during installation, so the operator can create a MachineInventory which will be using to bootstrap the node. See that we set the label that match our selector here already, although it can always be added later to the MachineInventory . :::warning warning Make sure to modify the registration.yaml above to set the proper install device to point to a valid device based on your node configuration(i.e. /dev/sda, /dev/vda, /dev/nvme0, etc...) ::: Now that we have all the configuration to create the proper resources in Kubernetes just apply them ```shell showLineNumbers kubectl apply -f selector.yaml kubectl apply -f cluster.yaml kubectl apply -f registration.yaml </TabItem> <TabItem value=\"repofiles\" label=\"Using quickstart files from Elemental repo directly\"> You can directly apply the quickstart example resource files from the [Elemental repository](https://github.com/rancher/elemental) :::warning warning This assumes that your Node will have a `/dev/sda` disk available as that is the default device selected in those files. If your node doesnt have that device you will have to manually create the registration.yaml file or download the one from the repo and modify before applying ::: ```bash showLineNumbers kubectl apply -f https://raw.githubusercontent.com/rancher/elemental/main/examples/quickstart/selector.yaml kubectl apply -f https://raw.githubusercontent.com/rancher/elemental/main/examples/quickstart/cluster.yaml kubectl apply -f https://raw.githubusercontent.com/rancher/elemental/main/examples/quickstart/registration.yaml","title":"Prepare you kubernetes resources"},{"location":"quickstart/#preparing-the-iso","text":"Now this is the last step, we need to prepare an Elemental Teal iso that includes the initial registration config, so it can be auto registered, installed and fully deployed as part of our cluster. The contents of the file are nothing more than the registration url that the node needs to register and the proper server certificate, so it can connect securely. This iso then can be used to provision an infinite number of machines Now, our MachineRegistration provides the needed config in its resource as part of its Status.RegistrationURL , so we can use that url to obtain the proper yaml needed for the iso. ``shell showLineNumbers wget --no-check-certificate kubectl get machineregistration -n fleet-default my-nodes -o jsonpath=\"{.status.registrationURL}\"` -O initial-registration.yaml This will download the proper yaml from the registration URL and store it on the current directory under the `initial-registration.yaml` name. </TabItem> <TabItem value=\"explanation\" label=\"Full explanation\"> First we need to obtain the `RegistrationURL` that was generated for our `MachineRegistration` ```bash showLineNumbers $ kubectl get machineregistration -n fleet-default my-test-registration -o jsonpath=\"{.status.registrationURL}\" https://172.18.0.2.sslip.io/elemental/registration/gsh4n8nj9gvbsjk4x7hxvnr5l6hmhbdbdffrmkwzrss2dtfbnpbmqp As you can see we obtained the proper initial registration needed by elemental-register to register the node properly and continue with the automated installation Then we need to visit that URL as that will provide the URL and CA certificate for unauthenticated requests: ```bash showLineNumbers $ curl --insecure https://172.18.0.2.sslip.io/elemental/registration/gsh4n8nj9gvbsjk4x7hxvnr5l6hmhbdbdffrmkwzrss2dtfbnpbmqp elemental: registration: url: https://172.18.0.2.sslip.io/elemental/registration/gsh4n8nj9gvbsjk4x7hxvnr5l6hmhbdbdffrmkwzrss2dtfbnpbmqp ca-cert: |- -----BEGIN CERTIFICATE----- MIIBqDCCAU2gAwIBAgIBADAKBggqhkjOPQQDAjA7MRwwGgYDVQQKExNkeW5hbWlj bGlzdGVuZXItb3JnMRswGQYDVQQDExJkeW5hbWljbGlzdGVuZXItY2EwHhcNMjIw ODA0MTA1OTE1WhcNMzIwODAxMTA1OTE1WjA7MRwwGgYDVQQKExNkeW5hbWljbGlz dGVuZXItb3JnMRswGQYDVQQDExJkeW5hbWljbGlzdGVuZXItY2EwWTATBgcqhkjO PQIBBggqhkjOPQMBBwNCAASa8PJH7JJGT5QUPMBYnJe0j50G7dTEaDlk4xRpqVk1 y4dloslsI0RTb6B++7nNgnLPOe2KqZfylNmVIAelrSaUo0IwQDAOBgNVHQ8BAf8E BAMCAqQwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUxp8OBfjZlnyV6pzzKqIF wWByvCYwCgYIKoZIzj0EAwIDSQAwRgIhAPI2XUWcnxkkBe98SGPFa1Hlncyu/FCR AbEYIAdUC2z+AiEA+GizukSRiiLV28wdNdKihEELy+qzi5MlVYowUuQYZsA= -----END CERTIFICATE----- As you can see we obtained the proper initial registration needed by `elemental-register` to register the node properly and continue with the automated installation Now we can write down the data returned for that url into a file that we will inject into the iso ```yaml title=\"initial-registration.yaml\" showLineNumbers elemental: registration: url: https://172.18.0.2.sslip.io/elemental/registration/gsh4n8nj9gvbsjk4x7hxvnr5l6hmhbdbdffrmkwzrss2dtfbnpbmqp ca-cert: |- -----BEGIN CERTIFICATE----- MIIBqDCCAU2gAwIBAgIBADAKBggqhkjOPQQDAjA7MRwwGgYDVQQKExNkeW5hbWlj bGlzdGVuZXItb3JnMRswGQYDVQQDExJkeW5hbWljbGlzdGVuZXItY2EwHhcNMjIw ODA0MTA1OTE1WhcNMzIwODAxMTA1OTE1WjA7MRwwGgYDVQQKExNkeW5hbWljbGlz dGVuZXItb3JnMRswGQYDVQQDExJkeW5hbWljbGlzdGVuZXItY2EwWTATBgcqhkjO PQIBBggqhkjOPQMBBwNCAASa8PJH7JJGT5QUPMBYnJe0j50G7dTEaDlk4xRpqVk1 y4dloslsI0RTb6B++7nNgnLPOe2KqZfylNmVIAelrSaUo0IwQDAOBgNVHQ8BAf8E BAMCAqQwDwYDVR0TAQH/BAUwAwEB/zAdBgNVHQ4EFgQUxp8OBfjZlnyV6pzzKqIF wWByvCYwCgYIKoZIzj0EAwIDSQAwRgIhAPI2XUWcnxkkBe98SGPFa1Hlncyu/FCR AbEYIAdUC2z+AiEA+GizukSRiiLV28wdNdKihEELy+qzi5MlVYowUuQYZsA= -----END CERTIFICATE----- Now we can proceed to create the ISO We provide a ISO build script for ease of use that can get the final ISO and inject the initial-registration.yaml : ```shell showLineNumbers wget -q https://raw.githubusercontent.com/rancher/elemental/main/.github/elemental-iso-add-registration && chmod +x elemental-iso-add-registration Now that we have the script we can proceed to download the ISO and inject our configuration injected: ```shell showLineNumbers ./elemental-iso-add-registration initial-registration.yaml This will generate an ISO on the current directory with the name elemental-teal-<ARCH>.iso :::info The script uses the iso for the arch based on the system is being run from. If you want to cross build for another system, you can set the ARCH environment variable to the desired target system (x86_64, aarch64) and the iso will be build for that architecture. ::: ```shell showLineNumbers wget -q https://raw.githubusercontent.com/rancher/elemental/main/elemental-iso-build && chmod +x elemental-iso-build Now that we have the script we can proceed to build the ISO with our configuration injected: ```shell showLineNumbers ./elemental-iso-build initial-registration.yaml This will generate an ISO on the current directory with the name elemental-<timestamp>.iso You can now boot your nodes with this ISO, and they will: Boot from the ISO Register with the registrationURL given and create a per-machine MachineInventory Install Elemental Teal to the given device Restart Auto-deploy the cluster via k3s After a few minutes your new cluster will be fully provisioned!!","title":"Preparing the iso"},{"location":"quickstart/#how-can-i-choose-the-kubernetes-version-and-deployer-for-the-cluster","text":"On you cluster.yaml file there is a key in the Spec called kubernetesVersion . That sets the version and deployer that will be used for the cluster, for example for rke v1.23.6 while for rke2 would be v1.23.6+rke2r1 and for k3s v1.23.6+k3s1 To see all compatible versions check the Rancher Support Matrix PDF for rke/rke2/k3s versions and their components. You can also check our Version doc to know how to obtain those versions. Check our Cluster Spec page for more info about the Cluster resource.","title":"How can I choose the kubernetes version and deployer for the cluster?"},{"location":"quickstart/#how-can-i-follow-what-is-going-on-behind-the-scenes","text":"You should be able to follow along what the machine is doing via: During ISO boot: ssh into the machine (user/pass: root/ros): running journalctl -f -t elemental will show you the output of the elemental-register and the elemental install Once the system is installed: On the Rancher UI -> Cluster Management you should see your new cluster and be able to see the Provisioning Log in the cluster details ssh into the machine (user/pass: Whatever your configured on the registration.yaml under Spec.config.cloud-config.users ): running journalctl -f -u elemental-system-agent will show the output of the initial elemental config and install of rancher-system-agent running journalctl -f -u rancher-system-agent will show the output of the boostrap of cluster components like k3s running journalctl -f -u k3s will show the logs of the k3s deployment","title":"How can I follow what is going on behind the scenes?"},{"location":"restore/","text":"Restore \u00b6 Follow this guide to restore an elemental node configuration from a backup with Rancher. Prepare rancher-backup operator and backup files for restoring \u00b6 Go to official Rancher documentation and make sure that rancher-bakup operator is installed and has access to backup files. Restore the elemental node configuration with rancher-backup operator \u00b6 Create a restore object to restore the backup tarball: ```yaml showLineNumbers apiVersion: resources.cattle.io/v1 kind: Restore metadata: name: restore-migration spec: backupFilename: rancher-backup-430169aa-edde-4a61-85e8-858f625a755b-2022-10-17T05-15-00Z.tar.gz Apply manifest on Kubernetes ```shell showLineNumbers kubectl apply -f rancher-restore.yaml Check logs from rancher-backup operator ```shell showLineNumbers kubectl logs -n cattle-resources-system -l app.kubernetes.io/name=rancher-backup -f Verify if backup file was restore successfully. ```shell showLineNumbers ... INFO[2022/10/31 06:34:50] Processing controllerRef apps/v1/deployments/rancher INFO[2022/10/31 06:34:50] Done restoring ... Continue with procedure from Rancher documentation","title":""},{"location":"restore/#restore","text":"Follow this guide to restore an elemental node configuration from a backup with Rancher.","title":"Restore"},{"location":"restore/#prepare-rancher-backup-operator-and-backup-files-for-restoring","text":"Go to official Rancher documentation and make sure that rancher-bakup operator is installed and has access to backup files.","title":"Prepare rancher-backup operator and backup files for restoring"},{"location":"restore/#restore-the-elemental-node-configuration-with-rancher-backup-operator","text":"Create a restore object to restore the backup tarball: ```yaml showLineNumbers apiVersion: resources.cattle.io/v1 kind: Restore metadata: name: restore-migration spec: backupFilename: rancher-backup-430169aa-edde-4a61-85e8-858f625a755b-2022-10-17T05-15-00Z.tar.gz Apply manifest on Kubernetes ```shell showLineNumbers kubectl apply -f rancher-restore.yaml Check logs from rancher-backup operator ```shell showLineNumbers kubectl logs -n cattle-resources-system -l app.kubernetes.io/name=rancher-backup -f Verify if backup file was restore successfully. ```shell showLineNumbers ... INFO[2022/10/31 06:34:50] Processing controllerRef apps/v1/deployments/rancher INFO[2022/10/31 06:34:50] Done restoring ... Continue with procedure from Rancher documentation","title":"Restore the elemental node configuration with rancher-backup operator"},{"location":"smbios/","text":"import Registration from \"!!raw-loader!../examples/quickstart/registration.yaml\" Introduction \u00b6 The System Management BIOS (SMBIOS) specification defines data structures (and access methods) that can be used to read management information produced by the BIOS of a computer. This allows us to gather hardware information about the running system and use that as part of our labels. How does Elemental uses SMBIOS data? \u00b6 The registration client tries to gather SMBIOS data by running dmidecode during the initial registration of the node and that data is sent to the registration controller to use on interpolating different fields in the inventory that we create for that node. Currently, we support interpolating that data into the machineName and the machineInventoryLabels of a machineRegistration The interpolation format is as follows: {$KEY/VALUE} and ${KEY/SUBKEY/VALUE} This can be mixed with normal strings so my-prefix-${KEY/VALUE} would result into the resolved values with my-prefix- prefixed For example, having the following SMBIOS data: console showLineNumbers System Information Manufacturer: My manufacturer Product Name: Awesome PC Version: Not Specified Serial Number: THX1138 Family: Toretto And setting the machineName to serial-${System Information/Serial Number} would result in the final value of serial-THX1138 This is useful to generate automatic names for machines based on their hardware values, for example using the UUID or the Product name. Our default machineName when the registration values are empty is \"m-${System Information/UUID}\" . :::warning warning All non-valid characters will be changed into - automatically on parse. Valid characters for labels are alphanumeric and - , _ and . For machineName the constraints are stricter as that value is used for the hostname so valid values are lowercase alphanumeric and - only. ::: A good use of SMBIOS data is to set up different labels for all your machines and get those values from the hardware directly. Having your machineInventoryLabels on the machineRegistration set to SMBIOS data would allow you to use selectors down the line to select similar machines. For example using the following label cpuFamily: \"${Processor Information/Family} would allow you to use a selector to search for i7 cpus in your machine fleet. {Registration}","title":""},{"location":"smbios/#introduction","text":"The System Management BIOS (SMBIOS) specification defines data structures (and access methods) that can be used to read management information produced by the BIOS of a computer. This allows us to gather hardware information about the running system and use that as part of our labels.","title":"Introduction"},{"location":"smbios/#how-does-elemental-uses-smbios-data","text":"The registration client tries to gather SMBIOS data by running dmidecode during the initial registration of the node and that data is sent to the registration controller to use on interpolating different fields in the inventory that we create for that node. Currently, we support interpolating that data into the machineName and the machineInventoryLabels of a machineRegistration The interpolation format is as follows: {$KEY/VALUE} and ${KEY/SUBKEY/VALUE} This can be mixed with normal strings so my-prefix-${KEY/VALUE} would result into the resolved values with my-prefix- prefixed For example, having the following SMBIOS data: console showLineNumbers System Information Manufacturer: My manufacturer Product Name: Awesome PC Version: Not Specified Serial Number: THX1138 Family: Toretto And setting the machineName to serial-${System Information/Serial Number} would result in the final value of serial-THX1138 This is useful to generate automatic names for machines based on their hardware values, for example using the UUID or the Product name. Our default machineName when the registration values are empty is \"m-${System Information/UUID}\" . :::warning warning All non-valid characters will be changed into - automatically on parse. Valid characters for labels are alphanumeric and - , _ and . For machineName the constraints are stricter as that value is used for the hostname so valid values are lowercase alphanumeric and - only. ::: A good use of SMBIOS data is to set up different labels for all your machines and get those values from the hardware directly. Having your machineInventoryLabels on the machineRegistration set to SMBIOS data would allow you to use selectors down the line to select similar machines. For example using the following label cpuFamily: \"${Processor Information/Family} would allow you to use a selector to search for i7 cpus in your machine fleet. {Registration}","title":"How does Elemental uses SMBIOS data?"},{"location":"tpm/","text":"Trusted Platform Module 2.0 (TPM) \u00b6 Trusted Platform Module (TPM, also known as ISO/IEC 11889) is an international standard for a secure cryptoprocessor, a dedicated microcontroller designed to secure hardware through integrated cryptographic keys. The term can also refer to a chip conforming to the standard. Add TPM module to virtual machine \u00b6 Easy way to add TPM to virtual machine is to use Libvirt with Virt-manager Create Virtual Machine \u00b6 After starting virt-manager create new virtual machine Verify and edit hardware module list \u00b6 On the hardware configuration screen, verify list of modules and click Add Hardware button Add TPM module to VM \u00b6 From the list of emulated devices choose TPM module and add it to VM Finish VM configuration \u00b6 On the last screen verify once again if TPM module was added properly Add TPM emulation to bare metal machine \u00b6 During applying MachineRegistration add following key to the yaml config:elemental:registration:emulate-tpm: true yaml title=\"registration-tpm.yaml\" showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: config: cloud-config: users: - name: root passwd: root elemental: install: reboot: true device: /dev/sda debug: true registration: emulate-tpm: true machineName: my-machine machineInventoryLabels: location: \"europe\"","title":"Trusted Platform Module 2.0 (TPM)"},{"location":"tpm/#trusted-platform-module-20-tpm","text":"Trusted Platform Module (TPM, also known as ISO/IEC 11889) is an international standard for a secure cryptoprocessor, a dedicated microcontroller designed to secure hardware through integrated cryptographic keys. The term can also refer to a chip conforming to the standard.","title":"Trusted Platform Module 2.0 (TPM)"},{"location":"tpm/#add-tpm-module-to-virtual-machine","text":"Easy way to add TPM to virtual machine is to use Libvirt with Virt-manager","title":"Add TPM module to virtual machine"},{"location":"tpm/#create-virtual-machine","text":"After starting virt-manager create new virtual machine","title":"Create Virtual Machine"},{"location":"tpm/#verify-and-edit-hardware-module-list","text":"On the hardware configuration screen, verify list of modules and click Add Hardware button","title":"Verify and edit hardware module list"},{"location":"tpm/#add-tpm-module-to-vm","text":"From the list of emulated devices choose TPM module and add it to VM","title":"Add TPM module to VM"},{"location":"tpm/#finish-vm-configuration","text":"On the last screen verify once again if TPM module was added properly","title":"Finish VM configuration"},{"location":"tpm/#add-tpm-emulation-to-bare-metal-machine","text":"During applying MachineRegistration add following key to the yaml config:elemental:registration:emulate-tpm: true yaml title=\"registration-tpm.yaml\" showLineNumbers apiVersion: elemental.cattle.io/v1beta1 kind: MachineRegistration metadata: name: my-nodes namespace: fleet-default spec: config: cloud-config: users: - name: root passwd: root elemental: install: reboot: true device: /dev/sda debug: true registration: emulate-tpm: true machineName: my-machine machineInventoryLabels: location: \"europe\"","title":"Add TPM emulation to bare metal machine"},{"location":"upgrade/","text":"import ClusterTarget from \"!!raw-loader!../examples/upgrade/upgrade-cluster-target.yaml\" import NodeSelector from \"!!raw-loader!../examples/upgrade/upgrade-node-selector.yaml\" import ManagedOSVersion from \"!!raw-loader!../examples/upgrade/upgrade-managedos-version.yaml\" import MangedOSVersionChannelJson from \"!!raw-loader!../examples/upgrade/managed-os-version-channel-json.yaml\" import ManagedOSVersionChannelCustom from \"!!raw-loader!../examples/upgrade/managed-os-version-channel-custom.yaml\" import Versions from \"../examples/upgrade/versions.raw!=!raw-loader!../examples/upgrade/versions.json\" Upgrade \u00b6 All components in Elemental are managed using Kubernetes. Below is how to use Kubernetes approaches to upgrade the components. Elemental Teal node upgrade \u00b6 Elemental Teal is upgraded with the Elemental Operator. Refer to the [Elemental Operator] documentation for complete information. There are two ways of selecting nodes for upgrading. Via a cluster target, which will match ALL nodes in a cluster that matches our selector or via node selector, which will match nodes based on the node labels. Node selector allows us to be more targeted with the upgrade while cluster selector just selects all the nodes in a matched cluster. You can target nodes for an upgrade via a clusterTarget by setting it to the cluster name that you want to upgrade. All nodes in a cluster that matches that name will match and be upgraded. {ClusterTarget} You can target nodes for an upgrade via a nodeSelector by setting it to the label and value that you want to match. Any nodes containing that key with the value will match and be upgraded. {NodeSelector} Selecting source for upgrade \u00b6 Just specify an OCI image on the osImage field {ClusterTarget} In this case we use the auto populated ManagedOSVersion resources to set the wanted managedOSVersionName field. See section Managing available versions to understand how the ManagedOSVersion are managed. {ManagedOSVersion} :::warning Warning If both osImage and ManagedOSVersion are defined in the same ManagedOSImage be aware that osImage takes precedence. ::: Managing available versions \u00b6 An ManagedOSVersionChannel resource can be created in a Kubernetes cluster where the Elemental operator is installed to synchronize available versions for upgrades. It has a syncer in order to generate ManagedOSVersion automatically. Currently, we provide a json syncer and a custom one. This syncer will fetch a json from url and parse it into valid ManagedOSVersion resources. {MangedOSVersionChannelJson} A custom syncer allows more flexibility on how to gather ManagedOSVersion by allowing custom commands with custom images. This type of syncer allows to run a given command with arguments and env vars in a custom image and output a json file to /data/output . The generated data is then automounted by the syncer and then parsed so it can gather create the proper versions. :::info The only requirement to make your own custom syncer is to make it output a json file to /data/output and keep the correct json structure. ::: See below for an example use of our discovery plugin , which gathers versions from either git or github releases. {ManagedOSVersionChannelCustom} In both cases the file that the operator expects to parse is a json file with the versions on it as follows {Versions}","title":""},{"location":"upgrade/#upgrade","text":"All components in Elemental are managed using Kubernetes. Below is how to use Kubernetes approaches to upgrade the components.","title":"Upgrade"},{"location":"upgrade/#elemental-teal-node-upgrade","text":"Elemental Teal is upgraded with the Elemental Operator. Refer to the [Elemental Operator] documentation for complete information. There are two ways of selecting nodes for upgrading. Via a cluster target, which will match ALL nodes in a cluster that matches our selector or via node selector, which will match nodes based on the node labels. Node selector allows us to be more targeted with the upgrade while cluster selector just selects all the nodes in a matched cluster. You can target nodes for an upgrade via a clusterTarget by setting it to the cluster name that you want to upgrade. All nodes in a cluster that matches that name will match and be upgraded. {ClusterTarget} You can target nodes for an upgrade via a nodeSelector by setting it to the label and value that you want to match. Any nodes containing that key with the value will match and be upgraded. {NodeSelector}","title":"Elemental Teal node upgrade"},{"location":"upgrade/#selecting-source-for-upgrade","text":"Just specify an OCI image on the osImage field {ClusterTarget} In this case we use the auto populated ManagedOSVersion resources to set the wanted managedOSVersionName field. See section Managing available versions to understand how the ManagedOSVersion are managed. {ManagedOSVersion} :::warning Warning If both osImage and ManagedOSVersion are defined in the same ManagedOSImage be aware that osImage takes precedence. :::","title":"Selecting source for upgrade"},{"location":"upgrade/#managing-available-versions","text":"An ManagedOSVersionChannel resource can be created in a Kubernetes cluster where the Elemental operator is installed to synchronize available versions for upgrades. It has a syncer in order to generate ManagedOSVersion automatically. Currently, we provide a json syncer and a custom one. This syncer will fetch a json from url and parse it into valid ManagedOSVersion resources. {MangedOSVersionChannelJson} A custom syncer allows more flexibility on how to gather ManagedOSVersion by allowing custom commands with custom images. This type of syncer allows to run a given command with arguments and env vars in a custom image and output a json file to /data/output . The generated data is then automounted by the syncer and then parsed so it can gather create the proper versions. :::info The only requirement to make your own custom syncer is to make it output a json file to /data/output and keep the correct json structure. ::: See below for an example use of our discovery plugin , which gathers versions from either git or github releases. {ManagedOSVersionChannelCustom} In both cases the file that the operator expects to parse is a json file with the versions on it as follows {Versions}","title":"Managing available versions"}]}